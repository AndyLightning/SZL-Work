{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVS_HW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyLightning/SZL-Work/blob/1/CVS_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0rvDQ2ZtmZ",
        "colab_type": "text"
      },
      "source": [
        "# **Computer Vision Systems Homework**\n",
        "\n",
        "Hi all!\n",
        "\n",
        "In the spirit of #StayAtHome I tried to assemble a homework project without leaving the house. So, if this looks a little DIY, that's why. So without further ado:\n",
        "\n",
        "## **Welcome to Cactusville!**\n",
        "\n",
        "Cactusville is a small town populated by - you guessed correcty - cacti. Since it is a rapidly developing village, they are considering to use self-driving vehicles in their hometown. Your job as a computer vision maestro is to develop the required detection methods.\n",
        "\n",
        "## The setting\n",
        "\n",
        "Cactusville is quite unique in the sense that the entire surface of the town is covered in blue tablecloth. The exact colour and pattern of the cloth may vary slightly.\n",
        "\n",
        "By-and large there are 3 different objects of interest:\n",
        "\n",
        "* **Cacti:** These are the inhabitants of the village, so self-driving cars must be able to detect them to avoid hitting a cactus. Cacti have four basic sub-types: ***Happy***, ***Sad***, ***Angry*** and ***Evil***\n",
        "* **Vehicles:** These are other vechiles you should also avoid colliding with. There are 3 vehicles in Cactusville: An ***SUV***, a ***truck***, and an ***airplane***.\n",
        "* **Traffic Signs:** There are several signs placed all around the town, often multiple ones on a single stand. There are 55 different traffic sign classes, which are not listed here for the sake of brevity.\n",
        "\n",
        "## Tasks\n",
        "\n",
        "The people of Cactusville provided 4 videos for you to develop your algorithms with. Each video consists of several RGB and corresponding depth frames, which are found in the '*rgb*' and '*depth*' subfolders of the video. They are ordered numerically. The depth image is a single-channel, 16-bit image, where the pixel value is the distance of that pixel from the camera in **mm**.\n",
        "\n",
        "The videos also contain a **calibration.yaml** file, which contains the intrinsic parameters of the camera. These are the same for all videos used, so feel free to hardcode the important values into your program.\n",
        "\n",
        "Your team has to complete the following tasks:\n",
        "\n",
        "1.   **Traditional Vision:** Create an algorithm to accurately detect and classify the 3 objects of interest (Cactus, Vehicle, Traffic Sign). You don't have to determine the subclass at this point.\n",
        "2.   **Deep Learning:** Use a deep learning algorithm to classify traffic signs. The package provided includes a training and validation database of 32x32 RGB images.\n",
        "3.   **3D Vision:** Determine the 3D positions of the object of interest relative to the camera. Use the center of an object's bounding box to determine the position on the image.\n",
        "\n",
        "## Hardcore Tasks\n",
        "\n",
        "There are also 3 hardcore tasks for those who like challenges. These aren't particularly difficult, but they take more work and require you to go a little bit beyond the scope of the practicals.\n",
        "\n",
        "1.   **Traditional Vision:** Determine the subclasses of Cacti and Vehicles\n",
        "2.   **Deep Learning:** Of the 55 possible traffic signs, 3 are missing from the training and test datasets. ('*X - Priority*', '*X - Turn left*', '*X - Turn right*') As a result, the neural net trained in task 2 will not be able to classify them properly. Extend your neural network to classify these as well.\n",
        "3.   **3D Vision:** Determine the absolute pose (4x4 transformation matrix) of the camera as it moves throughout the video. You can safely assume that the pose in the first frame of every video is the identity matrix.\n",
        "\n",
        "## Evaluation and Score\n",
        "\n",
        "The basic package also contains annotations (correct answers) in the file **annotations.pickle** and a small python script **evaluate.py** you can use to measure the performance of your algorithm. \n",
        "\n",
        "Your homework score will be computed using the same script, albeit on 2 secret videos that you were not provided. The reason for this is to make sure that your algorithm works in new situations as well. The secret videos use the same 2 tablecloths and 3 vehicles, but the traffic signs and the cacti may be different. Not to mention the illumination.\n",
        "\n",
        "The tasks will be evaluated using the following metrics:\n",
        "\n",
        "* Task 1 - **Average Precision** (AP): This metric is simply the average of **Recall** (nCorrect / nObject) and **Precision** (nCorrect / nPrediction).\n",
        "* Tasks 1 HC, 2 and 2 HC - **Classification accuracy**\n",
        "* Tasks 3 and 3 HC - **RBF error**: This is simple the squared error between the prediction and the correct answer transformed by an RBF (Radial Basis Function) kernel. This means that a perfect answer has a score of 1, a bad answer will result in a score close to 0.\n",
        "\n",
        "### **Answer format**\n",
        "\n",
        "The evaluation function takes a single argument: A dictionary that containes your predictions. On the top level this dictionary should look like this:\n",
        "\n",
        "```python\n",
        "myAnswers = {\n",
        "    'video1/rgb/1.jpg' : <<Predictions for the image>>,\n",
        "    'video1/rgb/2.jpg' : <<Predictions for the image>>,\n",
        "    ...\n",
        "    'video4/rgb/10.jpg' : <<Predictions for the image>>,\n",
        "}\n",
        "```\n",
        "It is important that the dictionary key contains the video path, since two videos might have image files with the same name. Also, include all images from all videos in the file (even if you have no predictions), since the evaluation function will look for them! The order of the images does not matter.\n",
        "\n",
        "A prediction for a single image should also be a dictionary with the following format:\n",
        "```python\n",
        "myPred = {\n",
        "    'poses' : [t_11, t12_, t_13, t_14, ..., t_33, t_34],\n",
        "    'objects' : [obj_1, obj_2, ... obj_n]\n",
        "}\n",
        "```\n",
        "The key `poses` contains the first three rows of the transformation matrix (the fourth row is always `[0 0 0 1]`). The key `objects` is a list, each element containing a single object prediction. The order of predictions does not matter. A single object prediction is also a list, containing the following elements:\n",
        "```python\n",
        "myObjects = [u, v, w, h, classInd, subClassInd, x, y, z]\n",
        "```\n",
        "\n",
        "* `(u, v)` are the center coordinates of the object's bounding box, while `(w, h)` are the width and height parameters. All four are expected in pixels @640x480 resolution.\n",
        "* `(x, y, z)` are the 3D coordinates of the object relative to the camera. They are expected in **meters**.\n",
        "* `classInd` is the index of the object class in the list `className` (see below). It is between 0 and 2.\n",
        "* `subClassInd` is the index of the subclass in the appropriate list in `subclassNames` (again, see below). It is between [0-54] for traffic signs, [0-2] for vehicles and [0-3] for cacti.\n",
        "\n",
        "## Rules\n",
        "\n",
        "Here are some important rules and guidelines you have to follow:\n",
        "\n",
        "*   This work is to be done in groups of 3 or 4 people. You can do it with less if you feel confident, but not more.\n",
        "*   Forming/finding a group is your job. Once you have one, 1 person from the group shold write me a message on teams with the names and neptun codes of the members.\n",
        "*   If you can't find a group by Sunday, write me and I'll formulate groups with the remaining people.\n",
        "*   The deadline for the submission is Friday midnight on the 14th week. You can make a late submission until the next Sunday midnight.\n",
        "*   You can opt out of the homework. In this case you will beed to take the midterm exam. This will be done via teams video chat (oral exam). If you want to take this option, write me a message by Sunday.\n",
        "*   To pass the homework, you will have to submit a working solution for the 3 basic tasks. The quality of your predictions has to be significantly better than what is achievable by random guessing.\n",
        "\n",
        "### Offered final grade\n",
        "\n",
        "To qualify for the offered final grade (and to skip the exams), you have to complete at least one of the hardcore tasks. What this final grade will be depends on the quality of the predictions. \n",
        "\n",
        "I cannot specify the criteria exactly at this time, since I don't know how easy/hard this homework is yet. I will, however adhere to the following guidelines:\n",
        "\n",
        "*   I'm planning to offer Good (4) and Excellent (5) final grades.\n",
        "*   Those, who completed all 3 hardcore tasks with high quality are gonna get a 5\n",
        "*   Those, who completed at least 2 hardcore tasks with high quality are gonna get **at least** a 4\n",
        "*   'High quality' is undefined to create a situation in which teams compete\n",
        "*   Also, I want to avoid two situations: a., where the criterion is so hard that only a few people manage to get an offered grade; and b., where it is so easy that everyone gets one.\n",
        "*   My goal is that about 40-50% of all students would get an offered grade, 15-20% getting 5, and 25-30% getting 4. These goals are might change if way more people opt out of homework than I expect.\n",
        "\n",
        "### Ethics\n",
        "\n",
        "Copying entire solutions from online sources or each other is plagiarism, and it will be checked using automated tools. There are things that are perfectly okay, such as:\n",
        "*   Copying small snippets (a few lines) from the OpenCV/PyTorch tutorials or stackoverflow, etc.\n",
        "*   Appropriating code from the practicals (you can copy the entire thing), especially the deep learning one.\n",
        "*   Since what is okay and what isn't is a bit subjective, if you are unsure, ask me.\n",
        "\n",
        "## So, how should we do this?\n",
        "\n",
        "So, how can you do this homework, especially if you haven't done things like this before? Here are a few tips:\n",
        "\n",
        "### Environment\n",
        "\n",
        "For development IDE the easiest is to just use Google Colab. To do this you just have to solve the homework inside this notebook. This is the simplest solution, although it has one drawback: the colab notebook has limited debugging capabilities.\n",
        "\n",
        "If you want something more powerful, I recommend the [PyCharm](https://www.jetbrains.com/pycharm/) IDE, which is a free and pretty powerful Python development tool.\n",
        "\n",
        "If you are planning to use PyCharm on Windows, you need to install a Python distribution, since Windows still doesn't come with one (it's 20 effing 20, Microsoft!). I recommend [Anaconda](https://www.anaconda.com/distribution/). Make sure you use Python 3.x and not 2.7.\n",
        "\n",
        "[Here's a tutorial on how to set it up.](https://www.youtube.com/watch?v=e53lRPmWrMI)\n",
        "\n",
        "### Collaboration within the team\n",
        "\n",
        "Since I would strongly discourage teams to collaborate physically in the current situation, I would recommend some methods for remote collaboration.\n",
        "\n",
        "* First of all, use Teams or similar methods to communicate.\n",
        "* Second, use git or a similar version control tool to handle multiple team members working on the same project. \n",
        "* I strongly recommend creating a private repository for your homework on [Github](https://github.com/) (since you can add exactly 3 collaborators - including you that's a 4 person team). There, you can also create issues and other nice-to-have features to track you development. Getting some experience with version control is an absolute must for any engineer anyways.\n",
        "\n",
        "Here's a tutorial for git for those who never used something like this before.\n",
        "\n",
        "To use git from a GUI, I recommend [SmartGit](https://www.syntevo.com/smartgit/) or [Git Extensions](http://gitextensions.github.io/).\n",
        "\n",
        "**ProTip:** If you use a Colab notebook, make sure to clear the output cells (especially figures and images) before you commit. Otherwise you'll litter in your repository.\n",
        "\n",
        "[Here is an introduction to git](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)\n",
        "\n",
        "### Making a submission\n",
        "\n",
        "You can make a submission at the appropriate page in the edu portal. The results and leaderboard will also be published here. The results are evaluated around 8pm (CET), so it's pointless to make multiple submission per day.\n",
        "\n",
        "**Note**: Your submission should be runnable from Colab or PyCharm (if you used any custom libraries, please note it), and it must include the trained neural network model file from task 2. Also, make sure that only the code required for evaluation is ran (you can use a control variable to skip training code).\n",
        "\n",
        "### Further resources\n",
        "\n",
        "[Python tutorials](https://docs.python.org/3/tutorial/)\n",
        "\n",
        "[OpenCV tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
        "\n",
        "[PyTorch tutorials](https://pytorch.org/tutorials/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWfyt5SdmUn",
        "colab_type": "text"
      },
      "source": [
        "# Solution\n",
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJi3x8AZtEV",
        "colab_type": "code",
        "outputId": "5723a64b-87d2-4327-fde7-1ed96254a8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "#Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip\n",
        "\n",
        "# Templates\n",
        "!wget https://github.com/AndyLightning/SZL-Work/raw/master/myTemplates1.zip\n",
        "!unzip -qq myTemplates1.zip\n",
        "!rm myTemplates1.zip\n",
        "\n",
        "!wget https://github.com/AndyLightning/SZL-Work/raw/master/myTemplates2.zip\n",
        "!unzip -qq myTemplates2.zip\n",
        "!rm myTemplates2.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-08 13:29:36--  http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
            "Resolving deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)... 152.66.243.112\n",
            "Connecting to deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)|152.66.243.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14450182 (14M) [application/zip]\n",
            "Saving to: ‘HW.zip’\n",
            "\n",
            "HW.zip              100%[===================>]  13.78M  5.62MB/s    in 2.5s    \n",
            "\n",
            "2020-04-08 13:29:41 (5.62 MB/s) - ‘HW.zip’ saved [14450182/14450182]\n",
            "\n",
            "--2020-04-08 13:29:45--  http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
            "Resolving deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)... 152.66.243.112\n",
            "Connecting to deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)|152.66.243.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175675617 (168M) [application/zip]\n",
            "Saving to: ‘trafficSignsHW.zip’\n",
            "\n",
            "trafficSignsHW.zip  100%[===================>] 167.54M  8.71MB/s    in 18s     \n",
            "\n",
            "2020-04-08 13:30:06 (9.50 MB/s) - ‘trafficSignsHW.zip’ saved [175675617/175675617]\n",
            "\n",
            "--2020-04-08 13:30:18--  https://github.com/AndyLightning/SZL-Work/raw/master/myTemplates1.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AndyLightning/SZL-Work/master/myTemplates1.zip [following]\n",
            "--2020-04-08 13:30:19--  https://raw.githubusercontent.com/AndyLightning/SZL-Work/master/myTemplates1.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9392655 (9.0M) [application/zip]\n",
            "Saving to: ‘myTemplates1.zip’\n",
            "\n",
            "myTemplates1.zip    100%[===================>]   8.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-04-08 13:30:19 (76.3 MB/s) - ‘myTemplates1.zip’ saved [9392655/9392655]\n",
            "\n",
            "--2020-04-08 13:30:23--  https://github.com/AndyLightning/SZL-Work/raw/master/myTemplates2.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/AndyLightning/SZL-Work/master/myTemplates2.zip [following]\n",
            "--2020-04-08 13:30:23--  https://raw.githubusercontent.com/AndyLightning/SZL-Work/master/myTemplates2.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18586536 (18M) [application/zip]\n",
            "Saving to: ‘myTemplates2.zip’\n",
            "\n",
            "myTemplates2.zip    100%[===================>]  17.72M  48.8MB/s    in 0.4s    \n",
            "\n",
            "2020-04-08 13:30:24 (48.8 MB/s) - ‘myTemplates2.zip’ saved [18586536/18586536]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSTAlH8td3eD",
        "colab_type": "text"
      },
      "source": [
        "## Folder example\n",
        "\n",
        "Get all subfolders in a directory\n",
        "\n",
        "```\n",
        "import os\n",
        "myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "```\n",
        "\n",
        "Get all files with extension in a directory\n",
        "\n",
        "```\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "names = sorted_nicely(glob.glob1(path, \"*.extension\"))\n",
        "```\n",
        "\n",
        "### Class names\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHMtqb8G7Lwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZdwqSb7dyZ",
        "colab_type": "text"
      },
      "source": [
        "### Display the first images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4663e9f0-cb4e-4c96-91fd-4af8039e3f2c",
        "id": "vRarrAXlq9ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "colors = [(0,0,255),(255,0,255),(0,255,0)]\n",
        "\n",
        "def drawBBs(BBs, img):\n",
        "    img = cv2.resize(img, (1280, 960))\n",
        "    for BB in BBs:\n",
        "        u = BB[0]*2\n",
        "        v = BB[1]*2\n",
        "        w = BB[2]*2\n",
        "        h = BB[3]*2\n",
        "        c = BB[4]\n",
        "        sc = BB[5]\n",
        "        x = BB[6]\n",
        "        y = BB[7]\n",
        "        z = BB[8]\n",
        "        s = (u - w // 2, v - h // 2)\n",
        "        e = (u + w // 2, v + h // 2)\n",
        "        cv2.rectangle(img, s, e, colors[c], 1)\n",
        "        tl = (s[0], s[1]+15)\n",
        "        bl = (s[0], e[1]-5)\n",
        "        cv2.putText(img,subclassNames[c][sc],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])\n",
        "        coords = \"(%.2f, %.2f, %.2f)\" % (x,y,z)\n",
        "        cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])\n",
        "    \n",
        "    return img\n",
        "\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Read images\n",
        "img = cv2.imread(\"HW/g1/rgb/1.jpg\")\n",
        "depth = cv2.imread(\"HW/g1/depth/1.png\", -1)\n",
        "\n",
        "# Read annotations\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "annotations = pickle.load(file)\n",
        "\n",
        "# Visualization\n",
        "depth = depth / 5000.0\n",
        "img = drawBBs(annotations[\"HW/g1/rgb/1.jpg\"][\"objects\"], img)\n",
        "img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Figure with subplots\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth,cmap='gray')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-5508bcb99084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Read annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HW/annotations.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HW/annotations.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CKrtjj4u9bR",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegTJSjhvI5S",
        "colab_type": "code",
        "outputId": "80a64047-02fd-40a1-9d4f-ae03307e7f2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        " \n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "# Read image names\n",
        "\n",
        "names1=sorted_nicely(glob.glob1('HW/g1/rgb/', \"*.jpg\"))\n",
        "names2=sorted_nicely(glob.glob1('HW/g2/rgb/', \"*.jpg\"))\n",
        "names3=sorted_nicely(glob.glob1('HW/g3/rgb/', \"*.jpg\"))\n",
        "names4=sorted_nicely(glob.glob1('HW/g4/rgb/', \"*.jpg\"))\n",
        "print(names1)\n",
        "#tesztkód\n",
        "\"\"\"\n",
        "path = 'trafficSignsHW/testFULL'\n",
        "dirs = os.listdir(path)\n",
        "tsNames=[]\n",
        "for i in dirs:\n",
        "  tsNames.append(sorted_nicely(glob.glob1('trafficSignsHW/testFULL/'+i, \"*.jpg\")))\n",
        "#print(tsNames)\n",
        "\"\"\"\n",
        "\n",
        "def MMaster(clasS, subClass, img2):\n",
        "  u=0\n",
        "  v=0\n",
        "  w=0\n",
        "  h=0\n",
        "  found=False\n",
        "  path=\"myTemplates/{}/{}/\"\n",
        "  imgNames = sorted_nicely(glob.glob1(path.format(clasS,subClass), \"*.PNG\"))\n",
        "  img1s = []\n",
        "  for i in imgNames:\n",
        "    img1s.append(cv2.cvtColor(cv2.imread(path.format(clasS,subClass)+i), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  gray1=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    gray1.append(cv2.cvtColor(img1s[cntr], cv2.COLOR_BGR2RGB))\n",
        "    cntr+=1\n",
        "  gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  \n",
        "  # ORB\n",
        "  #detector = cv2.ORB_create(2000)\n",
        "  detector=cv2.AKAZE_create()\n",
        "  #Kulcspontok és descriptorok meghatározása\n",
        "  kp1=[]\n",
        "  desc1=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    help1,help2 = detector.detectAndCompute(gray1[cntr], None)\n",
        "    kp1.append(help1)\n",
        "    desc1.append(help2)\n",
        "    cntr+=1\n",
        "  kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
        "\n",
        "  MIN_MATCH = 4 #minimum hány párosítás kell\n",
        "  if clasS==2:\n",
        "    MIN_MATCH=7\n",
        "  #Flann\n",
        "  FLANN_INDEX_LSH = 6\n",
        "\n",
        "  #Szótár - kulcs alapú konténer\n",
        "  index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
        "                    table_number = 6,\n",
        "                    key_size = 12,\n",
        "                    multi_probe_level = 1)\n",
        "  search_params=dict(checks=32)\n",
        "  #Létrehozzuk a párosító egyedet\n",
        "  #matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "  #matcher = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
        "  matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
        "\n",
        "  # Találjuk meg a 2 legjobb párosítást minden matchre\n",
        "  matches=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    matches.append(matcher.knnMatch(desc1[cntr], desc2, 2))\n",
        "    cntr+=1\n",
        "  #Paraméter a pontosságunkhoz\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ratio = 0.75\n",
        "  #Próbáld meg egy nagyon picit növelni, hátha.....\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  #Jó párosítások meghatározása\n",
        "  pre_matches=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    pre_matches.append([m[0] for m in matches[cntr] \\\n",
        "                      if len(m) == 2 and m[0].distance < m[1].distance * ratio])\n",
        "    cntr+=1\n",
        "\n",
        "  good_matches=len(pre_matches[0])\n",
        "  cntr=1\n",
        "  index=0\n",
        "  for i in imgNames[:-1]:\n",
        "    if len(pre_matches[cntr]) > good_matches:\n",
        "      good_matches=len(pre_matches[cntr])\n",
        "      index=cntr\n",
        "    cntr+=1\n",
        "  good_matches = [m[0] for m in matches[index] \\\n",
        "                      if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
        "  #Hány jó párosítást találtunk az \"összhalmazból\"\n",
        "  print('good matches:%d/%d' %(len(good_matches),len(matches[index])))\n",
        "\n",
        "  #0 értékekkel teli listát ad vissza, ami olyan hosszú, mint ahány jó párosításunk van\n",
        "  matchesMask = np.zeros(len(good_matches)).tolist()\n",
        "\n",
        "\n",
        "  #Ha több jó párosításunk van, mint ahányat megadunk a MIN_Match-el\n",
        "  if len(good_matches) > MIN_MATCH:\n",
        "      #Forrás és célpontok meghatározása\n",
        "      src_pts = np.float32([ kp1[index][m.queryIdx].pt for m in good_matches ])\n",
        "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
        "      #Megadja a perspektíva transzformációt (mtrx), és egy maszkot, ami tartalmazza az \"inlier\" és \"outlier\" pontokat\n",
        "      mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "      #Pontosság\n",
        "      accuracy=float(mask.sum())*100 / mask.size\n",
        "      print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
        "      #Ha a maszk elemeinek összege nagyobb MIN_MATCh-nél\n",
        "      if mask.sum() > MIN_MATCH:\n",
        "          found=True\n",
        "          # Sormátrixot csinálunk\n",
        "          matchesMask = mask.ravel().tolist()\n",
        "          #Template képnek a magassága és szélessége\n",
        "          h,w, = img1s[index].shape[:2]\n",
        "          #pts-nek megadjuk a template méret sarkait\n",
        "          pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
        "          #Célpont meghatározása\n",
        "          dst = cv2.perspectiveTransform(pts,mtrx)\n",
        "          u=(dst[0][0][0]+dst[1][0][0]+dst[2][0][0]+dst[3][0][0])/4\n",
        "          v=(dst[0][0][1]+dst[1][0][1]+dst[2][0][1]+dst[3][0][1])/4\n",
        "          w=((dst[3][0][0]-dst[0][0][0])+(dst[2][0][0]-dst[1][0][0]))/2\n",
        "          h=((dst[2][0][1]-dst[3][0][1])+(dst[1][0][1]-dst[0][0][1]))/2\n",
        "          #Kirajzoljuk a négyzetet körbe\n",
        "          img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
        "\n",
        "  #Összehúzott kép\n",
        "  res = cv2.drawMatches(img1s[index], kp1[index], img2, kp2, good_matches, None, \\\n",
        "                      matchesMask=matchesMask,\n",
        "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
        "  return res, found, u, v, w, h, clasS, subClass"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOTJfoULdtTy",
        "colab_type": "text"
      },
      "source": [
        "# Your Work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EXqOnb0b-M",
        "colab_type": "code",
        "outputId": "ea837fad-0a1b-4c48-bae4-abf6b4f11d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# Import\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imutils\n",
        "\n",
        "\n",
        "#Solution\n",
        "myAnswers={}\n",
        "myPred={}\n",
        "myObjects=[]\n",
        "\n",
        "# Read images\n",
        "imgs = []\n",
        "imgs_rgb = []\n",
        "imgs_gray = []\n",
        "imgs_red=[]\n",
        "mask = []\n",
        "masked_images = []\n",
        "\n",
        "#A destiny dictionaryben benen vannak a file nevek és az elérési útjuk\n",
        "allNames=[]\n",
        "cntr=0\n",
        "for i in names1:\n",
        "  allNames.append(names1[cntr])\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names2:\n",
        "  allNames.append(names2[cntr])\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names3:\n",
        "  allNames.append(names3[cntr])\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names4:\n",
        "  allNames.append(names4[cntr])\n",
        "  cntr+=1\n",
        "\n",
        "print(names1)\n",
        "allDestination=[]\n",
        "for i in names1:\n",
        "  allDestination.append(\"HW/g1/rgb/\"+i)\n",
        "for i in names2:\n",
        "  allDestination.append(\"HW/g2/rgb/\"+i)\n",
        "for i in names3:\n",
        "  allDestination.append(\"HW/g3/rgb/\"+i)\n",
        "for i in names4:\n",
        "  allDestination.append(\"HW/g4/rgb/\"+i)\n",
        "destiny = {\n",
        "  \"names\": allNames,\n",
        "  \"destination\": allDestination\n",
        "}\n",
        "names=destiny[\"names\"]\n",
        "print(destiny[\"destination\"])\n",
        "cntr=0\n",
        "for i in names:\n",
        "  imgs.append(cv2.imread(allDestination[cntr]))\n",
        "  cntr+=1\n",
        "\n",
        "print(imgs[0])\n",
        "#plt.imshow(imgs[0])\n",
        "\"\"\"\n",
        "# Convert images\n",
        "cntr = 0\n",
        "for i in names:\n",
        "  imgs_rgb.append(cv2.cvtColor(imgs[cntr], cv2.COLOR_BGR2RGB))\n",
        "  cntr+=1\n",
        "cntr = 0\n",
        "for i in names:\n",
        "  imgs_gray.append(cv2.cvtColor(imgs[cntr], cv2.COLOR_BGR2GRAY))\n",
        "  cntr+=1\n",
        "#Hist\n",
        "cntr = 0\n",
        "for i in names:\n",
        "  imgs_gray.append(cv2.equalizeHist(imgs_gray[cntr]))\n",
        "  cntr+=1  \n",
        "\n",
        "\n",
        "# Parameters + Masking\n",
        "lower_blue = np.array([0, 0, 50])     ##[R value, G value, B value]\n",
        "upper_blue = np.array([180, 180, 255])\n",
        "save_red_bot = np.array([75, 20, 0])\n",
        "save_red_top = np.array([255, 30, 20])\n",
        "\n",
        "\n",
        "\n",
        "#Get Reds\n",
        "cntr=0\n",
        "for i in names:\n",
        "  mask.append(cv2.inRange(imgs_rgb[cntr], save_red_bot, save_red_top))\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names:\n",
        "  masked_images.append(np.copy(imgs_rgb[cntr]))\n",
        "  cntr+=1  \n",
        "cntr=0\n",
        "for i in names:\n",
        "  x = masked_images[cntr]\n",
        "  x[mask[cntr] != 255] = [0, 0, 0]\n",
        "  masked_images[cntr]=x\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names:\n",
        "  x=masked_images[cntr]\n",
        "  imgs_red.append(x[:,:,0])\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names:\n",
        "  _, imgs_red[cntr]=(cv2.threshold(imgs_red[cntr], 100, 255, cv2.THRESH_BINARY))\n",
        "  cntr+=1\n",
        "#Get the faces\n",
        "\n",
        "\n",
        "# Plotting\n",
        "cntr=0\n",
        "plt.figure(figsize=(30,30))\n",
        "for i in names:\n",
        "  print(i)\n",
        "  print(cntr+1)\n",
        "  #plt.subplot(5,4,cntr+1)\n",
        "  #plt.imshow(MMaster(2,3,imgs_rgb[cntr]))\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(1,0,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(1,1,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(1,2,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(2,0,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(2,1,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(2,2,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  _,logic,u,v,w,h,ci,sci=MMaster(2,3,imgs_rgb[cntr])\n",
        "  if(logic):\n",
        "    myObjects.append([u,v,w,h,ci,sci,0,0,0])\n",
        "  myPred[\"objects\"] = myObjects\n",
        "  myPred[\"poses\"] = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  myAnswers[allDestination[cntr]]=myPred\n",
        "  cntr+=1\n",
        "  \n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1.jpg', '33.jpg', '60.jpg', '89.jpg', '123.jpg', '145.jpg', '159.jpg', '171.jpg', '186.jpg', '199.jpg', '213.jpg', '234.jpg', '258.jpg', '270.jpg', '295.jpg', '307.jpg', '326.jpg', '381.jpg', '394.jpg', '411.jpg']\n",
            "['HW/g1/rgb/1.jpg', 'HW/g1/rgb/33.jpg', 'HW/g1/rgb/60.jpg', 'HW/g1/rgb/89.jpg', 'HW/g1/rgb/123.jpg', 'HW/g1/rgb/145.jpg', 'HW/g1/rgb/159.jpg', 'HW/g1/rgb/171.jpg', 'HW/g1/rgb/186.jpg', 'HW/g1/rgb/199.jpg', 'HW/g1/rgb/213.jpg', 'HW/g1/rgb/234.jpg', 'HW/g1/rgb/258.jpg', 'HW/g1/rgb/270.jpg', 'HW/g1/rgb/295.jpg', 'HW/g1/rgb/307.jpg', 'HW/g1/rgb/326.jpg', 'HW/g1/rgb/381.jpg', 'HW/g1/rgb/394.jpg', 'HW/g1/rgb/411.jpg', 'HW/g2/rgb/1.jpg', 'HW/g2/rgb/47.jpg', 'HW/g2/rgb/64.jpg', 'HW/g2/rgb/108.jpg', 'HW/g2/rgb/126.jpg', 'HW/g2/rgb/147.jpg', 'HW/g2/rgb/185.jpg', 'HW/g2/rgb/205.jpg', 'HW/g2/rgb/236.jpg', 'HW/g2/rgb/257.jpg', 'HW/g2/rgb/286.jpg', 'HW/g2/rgb/313.jpg', 'HW/g2/rgb/329.jpg', 'HW/g2/rgb/359.jpg', 'HW/g2/rgb/383.jpg', 'HW/g2/rgb/415.jpg', 'HW/g3/rgb/1.jpg', 'HW/g3/rgb/65.jpg', 'HW/g3/rgb/77.jpg', 'HW/g3/rgb/136.jpg', 'HW/g3/rgb/153.jpg', 'HW/g3/rgb/189.jpg', 'HW/g3/rgb/208.jpg', 'HW/g3/rgb/225.jpg', 'HW/g3/rgb/250.jpg', 'HW/g3/rgb/278.jpg', 'HW/g3/rgb/292.jpg', 'HW/g3/rgb/307.jpg', 'HW/g3/rgb/327.jpg', 'HW/g3/rgb/340.jpg', 'HW/g3/rgb/356.jpg', 'HW/g3/rgb/376.jpg', 'HW/g3/rgb/409.jpg', 'HW/g3/rgb/420.jpg', 'HW/g3/rgb/441.jpg', 'HW/g4/rgb/1.jpg', 'HW/g4/rgb/38.jpg', 'HW/g4/rgb/62.jpg', 'HW/g4/rgb/78.jpg', 'HW/g4/rgb/95.jpg', 'HW/g4/rgb/116.jpg', 'HW/g4/rgb/134.jpg', 'HW/g4/rgb/152.jpg', 'HW/g4/rgb/159.jpg', 'HW/g4/rgb/190.jpg', 'HW/g4/rgb/210.jpg', 'HW/g4/rgb/229.jpg', 'HW/g4/rgb/250.jpg', 'HW/g4/rgb/270.jpg', 'HW/g4/rgb/297.jpg']\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Convert images\\ncntr = 0\\nfor i in names:\\n  imgs_rgb.append(cv2.cvtColor(imgs[cntr], cv2.COLOR_BGR2RGB))\\n  cntr+=1\\ncntr = 0\\nfor i in names:\\n  imgs_gray.append(cv2.cvtColor(imgs[cntr], cv2.COLOR_BGR2GRAY))\\n  cntr+=1\\n#Hist\\ncntr = 0\\nfor i in names:\\n  imgs_gray.append(cv2.equalizeHist(imgs_gray[cntr]))\\n  cntr+=1  \\n\\n\\n# Parameters + Masking\\nlower_blue = np.array([0, 0, 50])     ##[R value, G value, B value]\\nupper_blue = np.array([180, 180, 255])\\nsave_red_bot = np.array([75, 20, 0])\\nsave_red_top = np.array([255, 30, 20])\\n\\n\\n\\n#Get Reds\\ncntr=0\\nfor i in names:\\n  mask.append(cv2.inRange(imgs_rgb[cntr], save_red_bot, save_red_top))\\n  cntr+=1\\ncntr=0\\nfor i in names:\\n  masked_images.append(np.copy(imgs_rgb[cntr]))\\n  cntr+=1  \\ncntr=0\\nfor i in names:\\n  x = masked_images[cntr]\\n  x[mask[cntr] != 255] = [0, 0, 0]\\n  masked_images[cntr]=x\\n  cntr+=1\\ncntr=0\\nfor i in names:\\n  x=masked_images[cntr]\\n  imgs_red.append(x[:,:,0])\\n  cntr+=1\\ncntr=0\\nfor i in names:\\n  _, imgs_red[cntr]=(cv2.threshold(imgs_red[cntr], 100, 255, cv2.THRESH_BINARY))\\n  cntr+=1\\n#Get the faces\\n\\n\\n# Plotting\\ncntr=0\\nplt.figure(figsize=(30,30))\\nfor i in names:\\n  print(i)\\n  print(cntr+1)\\n  #plt.subplot(5,4,cntr+1)\\n  #plt.imshow(MMaster(2,3,imgs_rgb[cntr]))\\n  _,logic,u,v,w,h,ci,sci=MMaster(1,0,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(1,1,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(1,2,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(2,0,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(2,1,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(2,2,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  _,logic,u,v,w,h,ci,sci=MMaster(2,3,imgs_rgb[cntr])\\n  if(logic):\\n    myObjects.append([u,v,w,h,ci,sci,0,0,0])\\n  myPred[\"objects\"] = myObjects\\n  myPred[\"poses\"] = [0,0,0,0,0,0,0,0,0,0,0,0]\\n  myAnswers[allDestination[cntr]]=myPred\\n  cntr+=1\\n  \\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmTz3XRdwm1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This snippet assumes that the contents of the downloaded zip file are in the HW folder, and that your predictions are in a dictionary called predictions that adheres to the format specified above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRADdkYd0Er",
        "colab_type": "code",
        "outputId": "dee88b11-1525-4529-ab23-b8c7649b1740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from HW.evaluate import evaluate\n",
        "\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "predictions = pickle.load(file)\n",
        "predictions=myAnswers\n",
        "evaluate(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Task 1: 0.12499520764971866\n",
            "Task 1 HC: 0.4044943820224719\n",
            "Task 2: 0.0\n",
            "Task 2 HC: 0.0\n",
            "Task 3: 0.02687636426270428\n",
            "Task 3 HC: 0.20803167116939136\n",
            "Total:  0.7643976251042861\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}