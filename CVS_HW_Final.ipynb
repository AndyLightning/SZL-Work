{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CVS_HW with Deep Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aVZdwqSb7dyZ",
        "wWfbgkuVCCE4",
        "5CKrtjj4u9bR"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyLightning/SZL-Work/blob/Final-Final/CVS_HW_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul0rvDQ2ZtmZ",
        "colab_type": "text"
      },
      "source": [
        "# **Computer Vision Systems Homework**\n",
        "\n",
        "Hi all!\n",
        "\n",
        "In the spirit of #StayAtHome I tried to assemble a homework project without leaving the house. So, if this looks a little DIY, that's why. So without further ado:\n",
        "\n",
        "## **Welcome to Cactusville!**\n",
        "\n",
        "Cactusville is a small town populated by - you guessed correcty - cacti. Since it is a rapidly developing village, they are considering to use self-driving vehicles in their hometown. Your job as a computer vision maestro is to develop the required detection methods.\n",
        "\n",
        "## The setting\n",
        "\n",
        "Cactusville is quite unique in the sense that the entire surface of the town is covered in blue tablecloth. The exact colour and pattern of the cloth may vary slightly.\n",
        "\n",
        "By-and large there are 3 different objects of interest:\n",
        "\n",
        "* **Cacti:** These are the inhabitants of the village, so self-driving cars must be able to detect them to avoid hitting a cactus. Cacti have four basic sub-types: ***Happy***, ***Sad***, ***Angry*** and ***Evil***\n",
        "* **Vehicles:** These are other vechiles you should also avoid colliding with. There are 3 vehicles in Cactusville: An ***SUV***, a ***truck***, and an ***airplane***.\n",
        "* **Traffic Signs:** There are several signs placed all around the town, often multiple ones on a single stand. There are 55 different traffic sign classes, which are not listed here for the sake of brevity.\n",
        "\n",
        "## Tasks\n",
        "\n",
        "The people of Cactusville provided 4 videos for you to develop your algorithms with. Each video consists of several RGB and corresponding depth frames, which are found in the '*rgb*' and '*depth*' subfolders of the video. They are ordered numerically. The depth image is a single-channel, 16-bit image, where the pixel value is the distance of that pixel from the camera in **mm**.\n",
        "\n",
        "The videos also contain a **calibration.yaml** file, which contains the intrinsic parameters of the camera. These are the same for all videos used, so feel free to hardcode the important values into your program.\n",
        "\n",
        "Your team has to complete the following tasks:\n",
        "\n",
        "1.   **Traditional Vision:** Create an algorithm to accurately detect and classify the 3 objects of interest (Cactus, Vehicle, Traffic Sign). You don't have to determine the subclass at this point.\n",
        "2.   **Deep Learning:** Use a deep learning algorithm to classify traffic signs. The package provided includes a training and validation database of 32x32 RGB images.\n",
        "3.   **3D Vision:** Determine the 3D positions of the object of interest relative to the camera. Use the center of an object's bounding box to determine the position on the image.\n",
        "\n",
        "## Hardcore Tasks\n",
        "\n",
        "There are also 3 hardcore tasks for those who like challenges. These aren't particularly difficult, but they take more work and require you to go a little bit beyond the scope of the practicals.\n",
        "\n",
        "1.   **Traditional Vision:** Determine the subclasses of Cacti and Vehicles\n",
        "2.   **Deep Learning:** Of the 55 possible traffic signs, 3 are missing from the training and test datasets. ('*X - Priority*', '*X - Turn left*', '*X - Turn right*') As a result, the neural net trained in task 2 will not be able to classify them properly. Extend your neural network to classify these as well.\n",
        "3.   **3D Vision:** Determine the absolute pose (4x4 transformation matrix) of the camera as it moves throughout the video. You can safely assume that the pose in the first frame of every video is the identity matrix.\n",
        "\n",
        "## Evaluation and Score\n",
        "\n",
        "The basic package also contains annotations (correct answers) in the file **annotations.pickle** and a small python script **evaluate.py** you can use to measure the performance of your algorithm. \n",
        "\n",
        "Your homework score will be computed using the same script, albeit on 2 secret videos that you were not provided. The reason for this is to make sure that your algorithm works in new situations as well. The secret videos use the same 2 tablecloths and 3 vehicles, but the traffic signs and the cacti may be different. Not to mention the illumination.\n",
        "\n",
        "The tasks will be evaluated using the following metrics:\n",
        "\n",
        "* Task 1 - **Average Precision** (AP): This metric is simply the average of **Recall** (nCorrect / nObject) and **Precision** (nCorrect / nPrediction).\n",
        "* Tasks 1 HC, 2 and 2 HC - **Classification accuracy**\n",
        "* Tasks 3 and 3 HC - **RBF error**: This is simple the squared error between the prediction and the correct answer transformed by an RBF (Radial Basis Function) kernel. This means that a perfect answer has a score of 1, a bad answer will result in a score close to 0.\n",
        "\n",
        "### **Answer format**\n",
        "\n",
        "The evaluation function takes a single argument: A dictionary that containes your predictions. On the top level this dictionary should look like this:\n",
        "\n",
        "```python\n",
        "myAnswers = {\n",
        "    'video1/rgb/1.jpg' : <<Predictions for the image>>,\n",
        "    'video1/rgb/2.jpg' : <<Predictions for the image>>,\n",
        "    ...\n",
        "    'video4/rgb/10.jpg' : <<Predictions for the image>>,\n",
        "}\n",
        "```\n",
        "It is important that the dictionary key contains the video path, since two videos might have image files with the same name. Also, include all images from all videos in the file (even if you have no predictions), since the evaluation function will look for them! The order of the images does not matter.\n",
        "\n",
        "A prediction for a single image should also be a dictionary with the following format:\n",
        "```python\n",
        "myPred = {\n",
        "    'poses' : [t_11, t12_, t_13, t_14, ..., t_33, t_34],\n",
        "    'objects' : [obj_1, obj_2, ... obj_n]\n",
        "}\n",
        "```\n",
        "The key `poses` contains the first three rows of the transformation matrix (the fourth row is always `[0 0 0 1]`). The key `objects` is a list, each element containing a single object prediction. The order of predictions does not matter. A single object prediction is also a list, containing the following elements:\n",
        "```python\n",
        "myObjects = [u, v, w, h, classInd, subClassInd, x, y, z]\n",
        "```\n",
        "\n",
        "* `(u, v)` are the center coordinates of the object's bounding box, while `(w, h)` are the width and height parameters. All four are expected in pixels @640x480 resolution.\n",
        "* `(x, y, z)` are the 3D coordinates of the object relative to the camera. They are expected in **meters**.\n",
        "* `classInd` is the index of the object class in the list `className` (see below). It is between 0 and 2.\n",
        "* `subClassInd` is the index of the subclass in the appropriate list in `subclassNames` (again, see below). It is between [0-54] for traffic signs, [0-2] for vehicles and [0-3] for cacti.\n",
        "\n",
        "## Rules\n",
        "\n",
        "Here are some important rules and guidelines you have to follow:\n",
        "\n",
        "*   This work is to be done in groups of 3 or 4 people. You can do it with less if you feel confident, but not more.\n",
        "*   Forming/finding a group is your job. Once you have one, 1 person from the group shold write me a message on teams with the names and neptun codes of the members.\n",
        "*   If you can't find a group by Sunday, write me and I'll formulate groups with the remaining people.\n",
        "*   The deadline for the submission is Friday midnight on the 14th week. You can make a late submission until the next Sunday midnight.\n",
        "*   You can opt out of the homework. In this case you will beed to take the midterm exam. This will be done via teams video chat (oral exam). If you want to take this option, write me a message by Sunday.\n",
        "*   To pass the homework, you will have to submit a working solution for the 3 basic tasks. The quality of your predictions has to be significantly better than what is achievable by random guessing.\n",
        "\n",
        "### Offered final grade\n",
        "\n",
        "To qualify for the offered final grade (and to skip the exams), you have to complete at least one of the hardcore tasks. What this final grade will be depends on the quality of the predictions. \n",
        "\n",
        "I cannot specify the criteria exactly at this time, since I don't know how easy/hard this homework is yet. I will, however adhere to the following guidelines:\n",
        "\n",
        "*   I'm planning to offer Good (4) and Excellent (5) final grades.\n",
        "*   Those, who completed all 3 hardcore tasks with high quality are gonna get a 5\n",
        "*   Those, who completed at least 2 hardcore tasks with high quality are gonna get **at least** a 4\n",
        "*   'High quality' is undefined to create a situation in which teams compete\n",
        "*   Also, I want to avoid two situations: a., where the criterion is so hard that only a few people manage to get an offered grade; and b., where it is so easy that everyone gets one.\n",
        "*   My goal is that about 40-50% of all students would get an offered grade, 15-20% getting 5, and 25-30% getting 4. These goals are might change if way more people opt out of homework than I expect.\n",
        "\n",
        "### Ethics\n",
        "\n",
        "Copying entire solutions from online sources or each other is plagiarism, and it will be checked using automated tools. There are things that are perfectly okay, such as:\n",
        "*   Copying small snippets (a few lines) from the OpenCV/PyTorch tutorials or stackoverflow, etc.\n",
        "*   Appropriating code from the practicals (you can copy the entire thing), especially the deep learning one.\n",
        "*   Since what is okay and what isn't is a bit subjective, if you are unsure, ask me.\n",
        "\n",
        "## So, how should we do this?\n",
        "\n",
        "So, how can you do this homework, especially if you haven't done things like this before? Here are a few tips:\n",
        "\n",
        "### Environment\n",
        "\n",
        "For development IDE the easiest is to just use Google Colab. To do this you just have to solve the homework inside this notebook. This is the simplest solution, although it has one drawback: the colab notebook has limited debugging capabilities.\n",
        "\n",
        "If you want something more powerful, I recommend the [PyCharm](https://www.jetbrains.com/pycharm/) IDE, which is a free and pretty powerful Python development tool.\n",
        "\n",
        "If you are planning to use PyCharm on Windows, you need to install a Python distribution, since Windows still doesn't come with one (it's 20 effing 20, Microsoft!). I recommend [Anaconda](https://www.anaconda.com/distribution/). Make sure you use Python 3.x and not 2.7.\n",
        "\n",
        "[Here's a tutorial on how to set it up.](https://www.youtube.com/watch?v=e53lRPmWrMI)\n",
        "\n",
        "### Collaboration within the team\n",
        "\n",
        "Since I would strongly discourage teams to collaborate physically in the current situation, I would recommend some methods for remote collaboration.\n",
        "\n",
        "* First of all, use Teams or similar methods to communicate.\n",
        "* Second, use git or a similar version control tool to handle multiple team members working on the same project. \n",
        "* I strongly recommend creating a private repository for your homework on [Github](https://github.com/) (since you can add exactly 3 collaborators - including you that's a 4 person team). There, you can also create issues and other nice-to-have features to track you development. Getting some experience with version control is an absolute must for any engineer anyways.\n",
        "\n",
        "Here's a tutorial for git for those who never used something like this before.\n",
        "\n",
        "To use git from a GUI, I recommend [SmartGit](https://www.syntevo.com/smartgit/) or [Git Extensions](http://gitextensions.github.io/).\n",
        "\n",
        "**ProTip:** If you use a Colab notebook, make sure to clear the output cells (especially figures and images) before you commit. Otherwise you'll litter in your repository.\n",
        "\n",
        "[Here is an introduction to git](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)\n",
        "\n",
        "### Making a submission\n",
        "\n",
        "You can make a submission at the appropriate page in the edu portal. The results and leaderboard will also be published here. The results are evaluated around 8pm (CET), so it's pointless to make multiple submission per day.\n",
        "\n",
        "**Note**: Your submission should be runnable from Colab or PyCharm (if you used any custom libraries, please note it), and it must include the trained neural network model file from task 2. Also, make sure that only the code required for evaluation is ran (you can use a control variable to skip training code).\n",
        "\n",
        "### Further resources\n",
        "\n",
        "[Python tutorials](https://docs.python.org/3/tutorial/)\n",
        "\n",
        "[OpenCV tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
        "\n",
        "[PyTorch tutorials](https://pytorch.org/tutorials/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWfyt5SdmUn",
        "colab_type": "text"
      },
      "source": [
        "# Solution\n",
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpJi3x8AZtEV",
        "colab_type": "code",
        "outputId": "59ec9e71-579b-46a4-fdf2-1ef0b3593a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        }
      },
      "source": [
        "# Homework dataset\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
        "!unzip -qq HW.zip\n",
        "!rm HW.zip\n",
        "\n",
        "#Traffic Sign Classification set\n",
        "!wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
        "!unzip -qq trafficSignsHW.zip\n",
        "!rm trafficSignsHW.zip\n",
        "\n",
        "# Templates\n",
        "!wget https://github.com/AndyLightning/SZL-Work/raw/master/Faces.zip\n",
        "!unzip -qq Faces.zip\n",
        "!rm Faces.zip\n",
        "\n",
        "# Best Deep Learning Model\n",
        "!wget https://github.com/AndyLightning/SZL-Work/raw/Final/model_Final.pth"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 17:43:42--  http://deeplearning.iit.bme.hu/CVS/HW.zip\n",
            "Resolving deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)... 152.66.243.112\n",
            "Connecting to deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)|152.66.243.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14446460 (14M) [application/zip]\n",
            "Saving to: ‘HW.zip’\n",
            "\n",
            "HW.zip              100%[===================>]  13.78M  9.00MB/s    in 1.5s    \n",
            "\n",
            "2020-05-20 17:43:45 (9.00 MB/s) - ‘HW.zip’ saved [14446460/14446460]\n",
            "\n",
            "replace HW/annotations.pickle? [y]es, [n]o, [A]ll, [N]one, [r]ename: --2020-05-20 17:44:02--  http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip\n",
            "Resolving deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)... 152.66.243.112\n",
            "Connecting to deeplearning.iit.bme.hu (deeplearning.iit.bme.hu)|152.66.243.112|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 175675617 (168M) [application/zip]\n",
            "Saving to: ‘trafficSignsHW.zip’\n",
            "\n",
            "trafficSignsHW.zip   10%[=>                  ]  18.39M  11.1MB/s               ^C\n",
            "[trafficSignsHW.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of trafficSignsHW.zip or\n",
            "        trafficSignsHW.zip.zip, and cannot find trafficSignsHW.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4c9ecb7f5f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Traffic Sign Classification set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget http://deeplearning.iit.bme.hu/CVS/trafficSignsHW.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip -qq trafficSignsHW.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm trafficSignsHW.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSTAlH8td3eD",
        "colab_type": "text"
      },
      "source": [
        "## Folder example\n",
        "\n",
        "Get all subfolders in a directory\n",
        "\n",
        "```\n",
        "import os\n",
        "myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]\n",
        "```\n",
        "\n",
        "Get all files with extension in a directory\n",
        "\n",
        "```\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "names = sorted_nicely(glob.glob1(path, \"*.extension\"))\n",
        "```\n",
        "\n",
        "### Class names\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHMtqb8G7Lwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classNames = ['traffic sign', 'vehicle', 'cactus']\n",
        "subclassNames = [\n",
        "    ['Bump', 'Bumpy road', 'Bus stop', 'Children', 'Crossing (blue)', 'Crossing (red)', 'Cyclists',\n",
        "     'Danger (other)', 'Dangerous left turn', 'Dangerous right turn', 'Give way', 'Go ahead', 'Go ahead or left',\n",
        "     'Go ahead or right', 'Go around either way', 'Go around left', 'Go around right', 'Intersection', 'Limit 100',\n",
        "     'Limit 120', 'Limit 20', 'Limit 30', 'Limit 50', 'Limit 60', 'Limit 70', 'Limit 80', 'Limit 80 over',\n",
        "     'Limit over', 'Main road', 'Main road over', 'Multiple dangerous turns', 'Narrow road (left)',\n",
        "     'Narrow road (right)', 'No entry', 'No entry (both directions)', 'No entry (truck)', 'No stopping', 'No takeover',\n",
        "     'No takeover (truck)', 'No takeover (truck) end', 'No takeover end', 'No waiting', 'One way road',\n",
        "     'Parking', 'Road works', 'Roundabout', 'Slippery road', 'Stop', 'Traffic light', 'Train crossing',\n",
        "     'Train crossing (no barrier)', 'Wild animals', 'X - Priority', 'X - Turn left', 'X - Turn right'],\n",
        "    ['SUV','truck','plane'],\n",
        "    ['happy','sad','angry','evil']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZdwqSb7dyZ",
        "colab_type": "text"
      },
      "source": [
        "### Display the first images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vRarrAXlq9ce",
        "colab": {}
      },
      "source": [
        "\n",
        "colors = [(0,0,255),(255,0,255),(0,255,0)]\n",
        "\n",
        "def drawBBs(BBs, img):\n",
        "    img = cv2.resize(img, (1280, 960))\n",
        "    for BB in BBs:\n",
        "        u = BB[0]*2\n",
        "        v = BB[1]*2\n",
        "        w = BB[2]*2\n",
        "        h = BB[3]*2\n",
        "        c = BB[4]\n",
        "        sc = BB[5]\n",
        "        x = BB[6]\n",
        "        y = BB[7]\n",
        "        z = BB[8]\n",
        "        s = (max(0,u - w // 2),max(0,v - h // 2))\n",
        "        e = (u + w // 2,v + h // 2)\n",
        "        cv2.rectangle(img, s, e, colors[c], 1)\n",
        "        tl = (s[0], s[1]+15)\n",
        "        bl = (s[0], e[1]-5)\n",
        "        cv2.putText(img,subclassNames[c][sc],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])\n",
        "        coords = \"(%.2f, %.2f, %.2f)\" % (x,y,z)\n",
        "        cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])\n",
        "    \n",
        "    return img\n",
        "\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#This way it doesn't try to open a window un the GUI - works in python notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Read images\n",
        "img = cv2.imread(\"HW/g1/rgb/1.jpg\")\n",
        "depth = cv2.imread(\"HW/g1/depth/1.png\", -1)\n",
        "\n",
        "# Read annotations\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "annotations = pickle.load(file)\n",
        "\n",
        "# Visualization\n",
        "depth = depth / 5000.0\n",
        "img = drawBBs(annotations[\"HW/g1/rgb/1.jpg\"][\"objects\"], img)\n",
        "img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Figure with subplots\n",
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(depth,cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWfbgkuVCCE4",
        "colab_type": "text"
      },
      "source": [
        "#Célmappák Beállításához"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9s0zOXWCBjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imutils\n",
        "\n",
        " \n",
        "def sorted_nicely( l ):\n",
        "    \"\"\" Sort the given iterable in the way that humans expect.\"\"\"\n",
        "    convert = lambda text: int(text) if text.isdigit() else text\n",
        "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "    return sorted(l, key = alphanum_key)\n",
        "\n",
        "\n",
        "###Itt kéne állítani###\n",
        "names1=sorted_nicely(glob.glob1('HW/g1/rgb/', \"*.jpg\"))\n",
        "names2=sorted_nicely(glob.glob1('HW/g2/rgb/', \"*.jpg\"))\n",
        "names3=sorted_nicely(glob.glob1('HW/g3/rgb/', \"*.jpg\"))\n",
        "names4=sorted_nicely(glob.glob1('HW/g4/rgb/', \"*.jpg\"))\n",
        "\n",
        "allNames=[]\n",
        "allTruth=[]\n",
        "\n",
        "###Itt###\n",
        "cntr=0\n",
        "for i in names1:\n",
        "  allNames.append(names1[cntr])\n",
        "  allTruth.append(cntr<3)\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names2:\n",
        "  allNames.append(names2[cntr])\n",
        "  allTruth.append(cntr<3)\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names3:\n",
        "  allNames.append(names3[cntr])\n",
        "  allTruth.append(cntr<3)\n",
        "  cntr+=1\n",
        "cntr=0\n",
        "for i in names4:\n",
        "  allNames.append(names4[cntr])\n",
        "  allTruth.append(cntr<3)\n",
        "  cntr+=1\n",
        "\n",
        "###És itt###\n",
        "allDestination=[]\n",
        "for i in names1:\n",
        "  allDestination.append(\"HW/g1/rgb/\"+i)\n",
        "for i in names2:\n",
        "  allDestination.append(\"HW/g2/rgb/\"+i)\n",
        "for i in names3:\n",
        "  allDestination.append(\"HW/g3/rgb/\"+i)\n",
        "for i in names4:\n",
        "  allDestination.append(\"HW/g4/rgb/\"+i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CKrtjj4u9bR",
        "colab_type": "text"
      },
      "source": [
        "#Andris Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegTJSjhvI5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "###Descriptorok\n",
        "path=\"myTemplates/{}/{}/\"\n",
        "def getDescC(clasS,subClass):\n",
        "\n",
        "  img1s=[]\n",
        "  img1s.append(cv2.cvtColor(cv2.imread(path.format(2,0)+\"X.PNG\"), cv2.COLOR_BGR2RGB))\n",
        "  img1s.append(cv2.cvtColor(cv2.imread(path.format(2,1)+\"X.PNG\"), cv2.COLOR_BGR2RGB))\n",
        "  img1s.append(cv2.cvtColor(cv2.imread(path.format(2,2)+\"X.PNG\"), cv2.COLOR_BGR2RGB))\n",
        "  img1s.append(cv2.cvtColor(cv2.imread(path.format(2,3)+\"X.PNG\"), cv2.COLOR_BGR2RGB))\n",
        "  gray1=[]\n",
        "  cntr = 0\n",
        "  for i in img1s:\n",
        "    gray1.append(cv2.cvtColor(img1s[cntr], cv2.COLOR_BGR2RGB))\n",
        "    cntr+=1\n",
        "  detector=cv2.AKAZE_create()\n",
        "  #Kulcspontok és descriptorok meghatározása\n",
        "  kp1=[]\n",
        "  desc1=[]\n",
        "  cntr = 0\n",
        "  for i in img1s:\n",
        "    help1,help2 = detector.detectAndCompute(gray1[cntr], None)\n",
        "    kp1.append(help1)\n",
        "    desc1.append(help2)\n",
        "    cntr+=1\n",
        "  return kp1,desc1\n",
        "def getDesc(clasS,subClass):\n",
        "  imgNames = sorted_nicely(glob.glob1(path.format(clasS,subClass), \"*.PNG\"))\n",
        "  img1s=[]\n",
        "  for i in imgNames:\n",
        "      img1s.append(cv2.cvtColor(cv2.imread(path.format(clasS,subClass)+i), cv2.COLOR_BGR2RGB))\n",
        "  gray1=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    gray1.append(cv2.cvtColor(img1s[cntr], cv2.COLOR_BGR2RGB))\n",
        "    cntr+=1\n",
        "  can=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    can.append(cv2.Canny(gray1[cntr], 50,120))\n",
        "    cntr+=1\n",
        "  detector=cv2.AKAZE_create()\n",
        "  #Kulcspontok és descriptorok meghatározása\n",
        "  kp1=[]\n",
        "  desc1=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    help1,help2 = detector.detectAndCompute(can[cntr], None)\n",
        "    kp1.append(help1)\n",
        "    desc1.append(help2)\n",
        "    cntr+=1\n",
        "  return kp1,desc1\n",
        "\n",
        "\n",
        "kp00,desc00=getDesc(0,0)\n",
        "kp10,desc10=getDesc(1,0)\n",
        "kp11,desc11=getDesc(1,1)\n",
        "kp12,desc12=getDesc(1,2)\n",
        "kp20,desc20=getDesc(2,0)\n",
        "kp21,desc21=getDesc(2,1)\n",
        "kp22,desc22=getDesc(2,2)\n",
        "kp23,desc23=getDesc(2,3)\n",
        "kp24,desc24=getDesc(2,4)\n",
        "\n",
        "\n",
        "#Kontúrokat keresek\n",
        "def getArea(img):\n",
        "  contours, hierarchy = cv2.findContours(solution2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  area=0\n",
        "  for cnt in contours:\n",
        "    area+=cv2.contourArea(cnt)\n",
        "  return area\n",
        "\n",
        "def MMaster(clasS, subClass, img2): #####Friss kommentek vannak a gyorsítás miatt!!!!\n",
        "  u=0\n",
        "  v=0\n",
        "  w=0\n",
        "  h=0\n",
        "  found=False\n",
        "  path=\"myTemplates/{}/{}/\"\n",
        "  imgNames = sorted_nicely(glob.glob1(path.format(clasS,subClass), \"*.PNG\"))\n",
        "  img1s = []\n",
        "  for i in imgNames:\n",
        "    img1s.append(cv2.cvtColor(cv2.imread(path.format(clasS,subClass)+i), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "  gray1=[]\n",
        "  cntr = 0\n",
        "  #for i in imgNames:\n",
        "    #gray1.append(cv2.cvtColor(img1s[cntr], cv2.COLOR_BGR2RGB))\n",
        "    #cntr+=1\n",
        "  gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "  Canny2=cv2.Canny(gray2,50,120)\n",
        "  \"\"\"\n",
        "  if(clasS==2):\n",
        "    gray2=cv2.Canny(gray2,50,120)\n",
        "    cntr = 0\n",
        "    for i in imgNames:\n",
        "      gray1[cntr]=cv2.Canny(gray1[cntr],50,120)\n",
        "      cntr+=1\n",
        "  \"\"\"\n",
        "  # ORB\n",
        "  #detector = cv2.ORB_create(2000)\n",
        "  detector=cv2.AKAZE_create()\n",
        "  #Kulcspontok és descriptorok meghatározása\n",
        "  kp1=[]\n",
        "  desc1=[]\n",
        "  if (clasS==0):\n",
        "    kp1=kp00\n",
        "    desc1=desc00\n",
        "  elif (clasS==1):\n",
        "    if(subClass==0):\n",
        "      kp1=kp10\n",
        "      desc1=desc10\n",
        "    if(subClass==1):\n",
        "      kp1=kp11\n",
        "      desc1=desc11\n",
        "    if(subClass==2):\n",
        "      kp1=kp12\n",
        "      desc1=desc12\n",
        "  elif (clasS==2):\n",
        "    kp1=kp20\n",
        "    desc1=desc20\n",
        "  \n",
        "  kp2, desc2 = detector.detectAndCompute(Canny2, None)\n",
        "\n",
        "  MIN_MATCH = 4 #minimum hány párosítás kell\n",
        "  if clasS==2:\n",
        "    MIN_MATCH=7\n",
        "  #Flann\n",
        "  FLANN_INDEX_LSH = 6\n",
        "\n",
        "  #Szótár - kulcs alapú konténer\n",
        "  index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
        "                    table_number = 6,\n",
        "                    key_size = 12,\n",
        "                    multi_probe_level = 1)\n",
        "  search_params=dict(checks=32)\n",
        "  matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_BRUTEFORCE_HAMMING)\n",
        "\n",
        "  # Találjuk meg a 2 legjobb párosítást minden matchre\n",
        "  imgNames=[]\n",
        "  imgNames = sorted_nicely(glob.glob1(path.format(clasS,subClass), \"*.PNG\"))\n",
        "  matches=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    if(len(kp1[cntr])>=2 and len(kp2)>=2):\n",
        "      matches.append(matcher.knnMatch(desc1[cntr], desc2, 2))\n",
        "    else:\n",
        "      imgNames.remove(i)\n",
        "    cntr+=1\n",
        "  #Paraméter a pontosságunkhoz\n",
        "  if (len(matches)==0):\n",
        "    return img2,False,0,0,0,0,0,0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ratio = 0.75\n",
        "  #Próbáld meg egy nagyon picit növelni, hátha.....\n",
        "\n",
        "\n",
        "\n",
        "  #Nem stimmel:\n",
        "  \n",
        "  #Jó párosítások meghatározása\n",
        "  pre_matches=[]\n",
        "  cntr = 0\n",
        "  for i in imgNames:\n",
        "    pre_matches.append([m[0] for m in matches[cntr] \\\n",
        "                      if len(m) == 2 and m[0].distance < m[1].distance * ratio])\n",
        "    cntr+=1\n",
        "\n",
        "  good_matches=len(pre_matches[0])\n",
        "  cntr=1\n",
        "  index=0\n",
        "  for i in imgNames[:-1]:\n",
        "    if len(pre_matches[cntr]) > good_matches:\n",
        "      good_matches=len(pre_matches[cntr])\n",
        "      index=cntr\n",
        "    cntr+=1\n",
        "  good_matches = [m[0] for m in matches[index] \\\n",
        "                      if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
        "  #Hány jó párosítást találtunk az \"összhalmazból\"\n",
        "  print('good matches:%d/%d' %(len(good_matches),len(matches[index])))\n",
        "\n",
        "  #0 értékekkel teli listát ad vissza, ami olyan hosszú, mint ahány jó párosításunk van\n",
        "  matchesMask = np.zeros(len(good_matches)).tolist()\n",
        "\n",
        "\n",
        "  #Ha több jó párosításunk van, mint ahányat megadunk a MIN_Match-el\n",
        "  if len(good_matches) > MIN_MATCH:\n",
        "      #Forrás és célpontok meghatározása\n",
        "      src_pts = np.float32([ kp1[index][m.queryIdx].pt for m in good_matches ])\n",
        "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
        "      #Megadja a perspektíva transzformációt (mtrx), és egy maszkot, ami tartalmazza az \"inlier\" és \"outlier\" pontokat\n",
        "      mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
        "\n",
        "      #Pontosság\n",
        "      accuracy=float(mask.sum())*100 / mask.size\n",
        "      print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
        "      #Ha a maszk elemeinek összege nagyobb MIN_MATCh-nél\n",
        "      if mask.sum() > MIN_MATCH:\n",
        "          found=True\n",
        "          # Sormátrixot csinálunk\n",
        "          matchesMask = mask.ravel().tolist()\n",
        "          #Template képnek a magassága és szélessége\n",
        "          h,w, = img1s[index].shape[:2]\n",
        "          #pts-nek megadjuk a template méret sarkait\n",
        "          pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
        "          #Célpont meghatározása\n",
        "          dst = cv2.perspectiveTransform(pts,mtrx)\n",
        "          u=(dst[0][0][0]+dst[1][0][0]+dst[2][0][0]+dst[3][0][0])/4\n",
        "          v=(dst[0][0][1]+dst[1][0][1]+dst[2][0][1]+dst[3][0][1])/4\n",
        "          w=((dst[3][0][0]-dst[0][0][0])+(dst[2][0][0]-dst[1][0][0]))/2\n",
        "          h=((dst[2][0][1]-dst[3][0][1])+(dst[1][0][1]-dst[0][0][1]))/2\n",
        "          #Kirajzoljuk a négyzetet körbe\n",
        "          img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
        "          \n",
        "\n",
        "  #Összehúzott kép\n",
        "  res = cv2.drawMatches(img1s[index], kp1[index], img2, kp2, good_matches, None, \\\n",
        "                      matchesMask=matchesMask,\n",
        "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
        "  return res, found, np.int32(u), np.int32(v), np.int32(w), np.int32(h), clasS, index ###Átírtam, itt eredetileg subClass volt!!!\n",
        "\n",
        "\n",
        "def getContours(img,imgContour,minArea):\n",
        "  contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  cntr=0\n",
        "  for cnt in contours:\n",
        "    approx=[]\n",
        "    area=cv2.contourArea(cnt)\n",
        "    if area>minArea:\n",
        "      cv2.drawContours(imgContour, contours, -1, (255,0,255),7) \n",
        "      peri=cv2.arcLength(cnt,True)\n",
        "      approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      #cv2.rectangle(imgContour, (x , y ), (x + w , y + h ), (0, 255, 0), 5)\n",
        "      cntr+=1\n",
        "      #cv2.putText(imgContour, \"Points: \" + str(len(approx)), (x + w + 20, y + 20), cv2.FONT_HERSHEY_COMPLEX, .7,\n",
        "       #                 (0, 255, 0), 2)\n",
        "      #cv2.putText(imgContour, \"Area: \" + str(int(area)), (x + w + 20, y + 45), cv2.FONT_HERSHEY_COMPLEX, 0.7,\n",
        "       #                 (0, 255, 0), 2)\n",
        "  return cntr\n",
        "def getZone(img,imgContour,minArea):\n",
        "  contours,_ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  for cnt in contours:\n",
        "    area=cv2.contourArea(cnt)\n",
        "    if (area>minArea):\n",
        "      cv2.drawContours(imgContour, contours, -1, (255,255,255),cv2.FILLED) \n",
        "      peri=cv2.arcLength(cnt,True)\n",
        "      approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      print(area)\n",
        "  return opened  \n",
        "def getObjects(img,imgContour,minArea):\n",
        "  blur = cv2.GaussianBlur(img, (5,5), 0)\n",
        "  _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
        "  dilated = cv2.dilate(thresh, None, iterations=2)\n",
        "  opened = cv2.morphologyEx(dilated, cv2.MORPH_OPEN, np.ones((2,2))) #new\n",
        "  #imgs_canny=cv2.Canny(np.copy(opened),50,120) #new\n",
        "  #imgEro=cv2.erode(opened, kernel, iterations=1) #new\n",
        "  contours, _ = cv2.findContours(opened, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  #contours, hierarchy = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  cntr=0\n",
        "  ret1=[]\n",
        "  ret2=[]\n",
        "  ret3=[]\n",
        "  ret4=[]\n",
        "  for cnt in contours:\n",
        "    approx=[]\n",
        "    area=cv2.contourArea(cnt)\n",
        "    if (area>minArea and area <15000):\n",
        "      cv2.drawContours(imgContour, contours, -1, (255,0,255),1) \n",
        "      peri=cv2.arcLength(cnt,False)\n",
        "      approx = cv2.approxPolyDP(cnt, 0.02 * peri, False)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "\n",
        "      ret1.append(x)\n",
        "      ret2.append(y)\n",
        "      ret3.append(w)\n",
        "      ret4.append(h)\n",
        "\n",
        "      cv2.rectangle(imgContour, (x , y ), (x + w , y + h ), (0, 255, 0), 5)\n",
        "\n",
        "      cntr+=1\n",
        "  return ret1,ret2,ret3,ret4,(cntr-1)\n",
        "def isSomething(clas,subC,img):\n",
        "  bol=False\n",
        "  bad1=bad2=bad3=False\n",
        "  redParam=np.int32(0)\n",
        "  blackParam=np.int32(0)\n",
        "  area=np.int32(0)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  if(clas==1 and subC==0):\n",
        "    mask1=cv2.inRange(img2,np.array([60, 10, 0]), np.array([160, 30, 30]))\n",
        "    mask2=cv2.inRange(img2,np.array([0, 0, 0]), np.array([30, 30, 30]))\n",
        "    mask3=cv2.inRange(img2,np.array([160, 15, 0]), np.array([255, 65, 40]))#Too Bright\n",
        "  if(clas==1 and subC==2):\n",
        "    mask1=cv2.inRange(img2,np.array([100, 15, 0]), np.array([200, 65, 40])) #120 15\n",
        "    mask2=cv2.inRange(img2,np.array([0, 0, 0]), np.array([0, 0, 0]))\n",
        "  masksum=mask1+mask2\n",
        "  img3[masksum != [255]]=[0,0,0]\n",
        "  ###Kritériumterületeseknek:\n",
        "  if(clas==1 and subC==0):\n",
        "    imgcr[mask3 != [255]]=[0,0,0]\n",
        "    imgcr=cv2.morphologyEx(imgcr, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "    imgs_canny=cv2.Canny(imgcr,50,120)\n",
        "    dilationcr=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    (contours,hierarchy)=cv2.findContours(dilationcr,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      if(area>3000):\n",
        "        bad3=True\n",
        "        print(\"bad3-al van a baj\")\n",
        "    imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "    imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "    imgs_canny=cv2.Canny(imgcrit1,50,120)\n",
        "    dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    isZero=0\n",
        "    for pic, contour in enumerate(contours):\n",
        "      isZero+=1\n",
        "      area=cv2.contourArea(contour)\n",
        "      redParam=area\n",
        "      if(area<2700):\n",
        "        bad1=True\n",
        "      else:\n",
        "        bad1=False\n",
        "        break\n",
        "        print(\"Piros area kisebb 2700-mál\")\n",
        "    if (isZero==0):\n",
        "      bad1=True\n",
        "      print(\"Nincs piros kontúr\")\n",
        "    imgcrit2[mask2 != [255]]=[0,0,0]\n",
        "    imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)))\n",
        "    imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "    dilation2=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    x2=dilation2\n",
        "    contours=0\n",
        "    (contours,hierarchy)=cv2.findContours(dilation2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    isZero=0\n",
        "    for pic, contour in enumerate(contours):\n",
        "      isZero+=1\n",
        "      area=cv2.contourArea(contour)\n",
        "      blackParam=area\n",
        "      if(area<100):\n",
        "        bad2=True\n",
        "      else:\n",
        "        bad2=False\n",
        "        break\n",
        "        print(\"Fekete area kisebb 100-mál\")\n",
        "    if (isZero==0):\n",
        "      bad2=True\n",
        "      print(\"Nincs fekete kontúr\")\n",
        "  img3=cv2.morphologyEx(img3, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "  imgs_canny=cv2.Canny(img3,50,120)\n",
        "  dilation=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "  contours=0\n",
        "  (contours,hierarchy)=cv2.findContours(dilation,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  big=0\n",
        "  x=y=w=h=0\n",
        "  for pic, contour in enumerate(contours):\n",
        "    peri=cv2.arcLength(contour,True)\n",
        "    approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "    x , y , w, h = cv2.boundingRect(approx)\n",
        "    print(\"Nézzük:\")\n",
        "    if (w>(h*5) or h>(w*3)):\n",
        "      continue\n",
        "    print(\"Méret OK\")\n",
        "    if (bad1 or bad2 or bad3): ###Nem teljesült a kritériumkövetelmény\n",
        "      print(\"valami bad\")\n",
        "      continue\n",
        "    print(\"Színmennyiség OK\")\n",
        "    area=cv2.contourArea(contour)\n",
        "    if(area>3000):\n",
        "      if(big<area):\n",
        "        big=area\n",
        "      bol=True\n",
        "\n",
        "  return bol, dilation, big, np.int(x+w/2),np.int(y+h/2),np.int(w),np.int(h),dilation1,dilation2,redParam\n",
        "\n",
        "def isSUV(clas,subC,img):\n",
        "  bol=False\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0\n",
        "  big=0\n",
        "  m1=0\n",
        "  m2=0\n",
        "  m3=0\n",
        "  m4=0\n",
        "  p1=[]\n",
        "  p2=[]\n",
        "  p3=[]\n",
        "  p4=[]\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  if(clas==1 and subC==0):\n",
        "    mask3=cv2.inRange(np.copy(img2),np.array([200, 15, 0]), np.array([255, 65, 40]))#Too Bright mask3=cv2.inRange(np.copy(img2),np.array([140, 15, 0]), np.array([255, 65, 40]))\n",
        "  if(clas==1 and subC==2):\n",
        "    mask1=cv2.inRange(img2,np.array([100, 15, 0]), np.array([200, 65, 40])) #120 15\n",
        "    mask2=cv2.inRange(img2,np.array([0, 0, 0]), np.array([0, 0, 0]))\n",
        "  #masksum=mask1+mask2\n",
        "  #img3[masksum != [255]]=[0,0,0]\n",
        "  ###Kritériumterületeseknek:\n",
        "  if(clas==1 and subC==0):\n",
        "    #Pirosság\n",
        "    imgcrit1=np.copy(img1)\n",
        "    imafterKill=np.copy(img1)\n",
        "    mask1=cv2.inRange(imgcrit1,np.array([85, 4, 0]), np.array([200, 50, 30])) #mask1=cv2.inRange(imgcrit1,np.array([75, 10, 10]), np.array([200, 50, 30]))\n",
        "    imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "    imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "    imgs_canny=cv2.Canny(imgcrit1,50,120)\n",
        "    dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      redParam=area\n",
        "      #print(\"redParam: \", redParam)\n",
        "      if(area>3000):\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        p1.append(x)\n",
        "        p2.append(y)\n",
        "        p3.append(x+w)\n",
        "        p4.append(y+h)\n",
        "        cv2.rectangle(img2, (x , y ), (x + w , y + h ), (255, 255, 255), -1)\n",
        "    img2=cv2.inRange(img2,np.array([250, 250, 250]), np.array([255, 255, 255]))\n",
        "    img1[img2 != [255]]=[0,0,0]\n",
        "    #Feketeség -> Lesz Fekete Masksum\n",
        "    imgcrit2=np.copy(img1)\n",
        "    mask21=cv2.inRange(np.copy(imgcrit2),np.array([0, 0, 0]), np.array([30, 30, 30]))\n",
        "    mask22=cv2.inRange(np.copy(imgcrit2),np.array([30, 30, 30]), np.array([60, 60, 90]))\n",
        "    mask23=cv2.inRange(np.copy(imgcrit2),np.array([60, 60, 60]), np.array([90, 60, 90]))\n",
        "    mask24=cv2.inRange(np.copy(imgcrit2),np.array([90, 90, 90]), np.array([120, 120, 120]))\n",
        "    masksum=mask21+mask22+mask23+mask24\n",
        "    imgcrit2[masksum != [255]]=[0]\n",
        "    imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)))\n",
        "    imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "    dilation2=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      blackParam=area\n",
        "      if(area>1400):\n",
        "        #print(\"blackParam: \", blackParam)\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        k1=x+w//2\n",
        "        k2=y+h//2\n",
        "        cntr=0\n",
        "        for i in p1:\n",
        "          if(p1[cntr] < k1 and k1< p3[cntr] and p2[cntr] < k2 and k2 < p4[cntr] and p2[cntr]!=0):\n",
        "            cv2.rectangle(img, (p1[cntr]-20 , p2[cntr]-20), (p3[cntr]+20,p4[cntr]+20), (255, 255, 255), -1) #Itt\n",
        "            m1=(p1[cntr]+p3[cntr])//2\n",
        "            m2=(p2[cntr]+p4[cntr])//2\n",
        "            m3=(p3[cntr]-p1[cntr])\n",
        "            m4=(p4[cntr]-p2[cntr])\n",
        "            bol=True\n",
        "            break            \n",
        "          cntr+=1\n",
        "    return bol, img, big, m1,m2,m3,m4,img1,dilation1,dilation2,blackParam\n",
        "def isTruck(clas,subC,img):\n",
        "  bol=False\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0\n",
        "  big=0\n",
        "  m1=0\n",
        "  m2=0\n",
        "  m3=0\n",
        "  m4=0\n",
        "  p1=[]\n",
        "  p2=[]\n",
        "  p3=[]\n",
        "  p4=[]\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  if(clas==1 and subC==1):\n",
        "    #Sárgaság\n",
        "    imgcrit1=np.copy(img1)\n",
        "    imafterKill=np.copy(img1)\n",
        "    mask1=cv2.inRange(imgcrit1,np.array([140, 80, 0]), np.array([255, 200, 25])) #mask1=cv2.inRange(imgcrit1,np.array([140, 80, 0]), np.array([255, 200, 30]))\n",
        "    imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "    imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "    imgs_canny=cv2.Canny(imgcrit1,50,120)\n",
        "    dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      if(area>5000):\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        p1.append(x)\n",
        "        p2.append(y)\n",
        "        p3.append(x+w)\n",
        "        p4.append(y+h)\n",
        "        cv2.rectangle(img2, (x , y -20), (x + w , y + h +20), (255, 255, 255), -1)\n",
        "    #Sárgaság\n",
        "    img2=cv2.inRange(img2,np.array([250, 250, 250]), np.array([255, 255, 255]))\n",
        "    img1[img2 != [255]]=[0,0,0]\n",
        "    imgcrit2=np.copy(img1)\n",
        "    mask2=cv2.inRange(np.copy(imgcrit2),np.array([0, 0, 0]), np.array([30, 30, 30]))\n",
        "    imgcrit2[mask2 != [255]]=[0]\n",
        "    imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)))\n",
        "    imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "    dilation2=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      blackParam=area\n",
        "      if(area>1000):\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        k1=x+w//2\n",
        "        k2=y+h//2\n",
        "        cntr=0\n",
        "        for i in p1:\n",
        "          if(p1[cntr] < k1 and k1< p3[cntr] and p2[cntr]-20 < k2 and k2 < (p4[cntr]+30) and p2[cntr]!=0 and k2>200):\n",
        "            cv2.rectangle(img, (p1[cntr] -20, p2[cntr]-20), (p3[cntr]+20,p4[cntr]+20), (255, 255,255), -1) #Itt\n",
        "            m1=(p1[cntr]+p3[cntr])//2\n",
        "            m2=(p2[cntr]+p4[cntr])//2\n",
        "            m3=(p3[cntr]-p1[cntr])\n",
        "            m4=(p4[cntr]-p2[cntr])\n",
        "            bol=True\n",
        "            break            \n",
        "          cntr+=1\n",
        "    return bol, img, big, m1,m2,m3,m4,img1,dilation1,dilation2,blackParam\n",
        "\n",
        "\n",
        "def isTable(clas,subC,img):\n",
        "  bol=False\n",
        "  newOpen=False\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0\n",
        "  big=0\n",
        "  m1=[]\n",
        "  m2=[]\n",
        "  m3=[]\n",
        "  m4=[]\n",
        "  p1=[]\n",
        "  p2=[]\n",
        "  p3=[]\n",
        "  p4=[]\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  if(clas==0):\n",
        "    #White Everywhere\n",
        "    imgcrit1=np.copy(img1)\n",
        "    imafterKill=np.copy(img1)\n",
        "    mask1=cv2.inRange(imgcrit1,np.array([115, 100, 60]), np.array([160, 145, 140])) #mask1=cv2.inRange(imgcrit1,np.array([115, 100, 60]), np.array([200, 215, 215]))\n",
        "    imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "    imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(14,14)))\n",
        "    imgs_canny=cv2.Canny(imgcrit1,50,120)\n",
        "    dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      if(area>3000):\n",
        "        print(area)\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        p1.append(x)\n",
        "        p2.append(y)\n",
        "        p3.append(x+w)\n",
        "        p4.append(y+h)\n",
        "        cv2.rectangle(img2, (x , y), (x + w , y + h), (255, 255, 255), -1)\n",
        "    #Keressünk táblákat\n",
        "    img2=cv2.inRange(img2,np.array([250, 250, 250]), np.array([255, 255, 255])) \n",
        "    img1[img2 != [255]]=[0,0,0]\n",
        "    imgcrit2=np.copy(img1)\n",
        "    img3=np.copy(img1)\n",
        "    mask21=cv2.inRange(np.copy(imgcrit2),np.array([100, 20, 30]), np.array([200, 70, 50]))\n",
        "    mask22=cv2.inRange(np.copy(imgcrit2),np.array([0, 40, 10]), np.array([30, 90, 40]))\n",
        "    mask23=cv2.inRange(np.copy(imgcrit2),np.array([120, 100, 0]), np.array([200, 160, 20]))\n",
        "    masksum=[mask21,mask22,mask23]\n",
        "    cntrj=1\n",
        "    for j in masksum:\n",
        "      imgcrit2=np.copy(img3)\n",
        "      imgcrit2[j != [255]]=[0]\n",
        "      imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "      imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "      dilation2=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "      contours=[]\n",
        "      (contours,hierarchy)=cv2.findContours(dilation2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "      for pic, contour in enumerate(contours):\n",
        "        area=cv2.contourArea(contour)\n",
        "        blackParam=area\n",
        "        if(area>800 and area<3500):\n",
        "          print(\"Area OK\")\n",
        "          peri=cv2.arcLength(contour,True)\n",
        "          approx = cv2.approxPolyDP(contour, 0.1 * peri, True) ###Ezt átírtam 0.02-ről!!!\n",
        "          x , y , w, h = cv2.boundingRect(approx)\n",
        "          k1=x+w//2\n",
        "          k2=y+h//2\n",
        "          cntr=0\n",
        "          for i in p1:\n",
        "            if(p1[cntr] < k1 and k1< p3[cntr] and p2[cntr] < k2 and k2 < (p4[cntr]) and p2[cntr]!=0):\n",
        "              cv2.rectangle(img1, (x,y), (x+w//2,y+h//2), (0, 255, 255), 5)\n",
        "              m1.append((p1[cntr]+p3[cntr])//2)\n",
        "              m2.append((p2[cntr]+p4[cntr])//2)\n",
        "              m3.append(p3[cntr]-p1[cntr])\n",
        "              m4.append(p4[cntr]-p2[cntr])\n",
        "              bol=True            \n",
        "            cntr+=1\n",
        "    return bol, img, big, m1,m2,m3,m4,img1,dilation1,dilation2,blackParam\n",
        "\n",
        "def isReallyCactus(x,y,r,img):\n",
        "  bol=False\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0\n",
        "  big=0\n",
        "  m1=x\n",
        "  m2=y\n",
        "  m3=2*r\n",
        "  m4=2*r\n",
        "  p1=0\n",
        "  p2=0\n",
        "  p3=0\n",
        "  p4=0\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  #Valami zöldes trutymó\n",
        "  imgcrit1=np.copy(img1)\n",
        "  cv2.rectangle(imgcrit1, (x-60 , y-60), (x + 60 , y + 60), (255, 255, 255), -1)\n",
        "  imgcrit1=cv2.inRange(imgcrit1,np.array([250,250,250]), np.array([255,255,255]))\n",
        "  img3[imgcrit1==[0]]=[0,0,0]\n",
        "  mask1=cv2.inRange(img3,np.array([20, 20, 0]), np.array([70, 70, 30])) #mask1=cv2.inRange(imgcrit1,np.array([140, 80, 0]), np.array([255, 200, 30]))\n",
        "  imgcrit2[mask1 != [255]]=[0,0,0]\n",
        "  imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)))\n",
        "  imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "  dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "  contours=[]\n",
        "  (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  for pic, contour in enumerate(contours):\n",
        "    area=cv2.contourArea(contour)\n",
        "    if(area>800): #800-al egész jó volt\n",
        "      #print(area)\n",
        "      peri=cv2.arcLength(contour,True)\n",
        "      approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      p1=min(x,m1-m3//2)\n",
        "      p2=min(y,m2-m4//2)\n",
        "      p3=max(x+w,m1+m3//2)\n",
        "      p4=max(y+h+20,m2+20+m4//2)\n",
        "      cv2.rectangle(img, (p1-10, p2-30), (p3+10 , p4+20), (255, 255, 255), -1) #Itt\n",
        "      bol=True\n",
        "  return bol, img, big, (p1+p3)//2,(p2+p4)//2,(p3-p1),(p4-p2),img1,dilation1,imgcrit1,blackParam\n",
        "\n",
        "\n",
        "def isRepzi(clas,subC,img):\n",
        "  bol=False\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0\n",
        "  big=0\n",
        "  m1=0\n",
        "  m2=0\n",
        "  m3=0\n",
        "  m4=0\n",
        "  p1=[]\n",
        "  p2=[]\n",
        "  p3=[]\n",
        "  p4=[]\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  if(clas==1 and subC==2):\n",
        "    mask1=cv2.inRange(img2,np.array([100, 15, 0]), np.array([200, 65, 40])) #120 15\n",
        "    mask2=cv2.inRange(img2,np.array([170, 70, 0]), np.array([200, 140, 50]))\n",
        "    imgcrit1=np.copy(img1)\n",
        "    imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "    imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)))\n",
        "    imgs_canny=cv2.Canny(imgcrit1,50,120)\n",
        "    dilation1=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      redParam=area\n",
        "      if(area>3000):\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        p1.append(x)\n",
        "        p2.append(y)\n",
        "        p3.append(x+w)\n",
        "        p4.append(y+h)\n",
        "        cv2.rectangle(img2, (x , y ), (x + w , y + h ), (255, 255, 255), -1)\n",
        "    img2=cv2.inRange(img2,np.array([250, 250, 250]), np.array([255, 255, 255]))\n",
        "    img1[img2 != [255]]=[0,0,0]\n",
        "    imgcrit2=np.copy(img1)\n",
        "    imgcrit2[mask2 != [255]]=[0]\n",
        "    imgcrit2=cv2.morphologyEx(imgcrit2, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2)))\n",
        "    imgs_canny=cv2.Canny(imgcrit2,50,120)\n",
        "    dilation2=cv2.dilate(imgs_canny,kernel_dil,iterations=1)\n",
        "    contours=[]\n",
        "    (contours,hierarchy)=cv2.findContours(dilation2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    for pic, contour in enumerate(contours):\n",
        "      area=cv2.contourArea(contour)\n",
        "      blackParam=area\n",
        "      if(area>400 and area<3000):\n",
        "        peri=cv2.arcLength(contour,True)\n",
        "        approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "        x , y , w, h = cv2.boundingRect(approx)\n",
        "        k1=x+w//2\n",
        "        k2=y+h//2\n",
        "        cntr=0\n",
        "        for i in p1:\n",
        "          if(p1[cntr] < k1 and k1< p3[cntr] and p2[cntr] < k2 and k2 < p4[cntr] and p2[cntr]!=0):\n",
        "            cv2.rectangle(img, (p1[cntr] , p2[cntr]), (p3[cntr],p4[cntr]), (255, 255, 255), -1) #Itt\n",
        "            m1=(p1[cntr]+p3[cntr])//2\n",
        "            m2=(p2[cntr]+p4[cntr])//2\n",
        "            m3=(p3[cntr]-p1[cntr])\n",
        "            m4=(p4[cntr]-p2[cntr])\n",
        "            bol=True\n",
        "            break            \n",
        "          cntr+=1\n",
        "    return bol, img, big, m1,m2,m3,m4,img1,dilation1,dilation2,blackParam\n",
        "def concentrate(num):\n",
        "  if num<0:\n",
        "    return 0\n",
        "  if num>700:\n",
        "    return 0\n",
        "  return num\n",
        "def redTable (img,obje):\n",
        "  bol=True\n",
        "  redParam=0\n",
        "  blackParam=0\n",
        "  area=0.0\n",
        "  big=0\n",
        "  m1=[]\n",
        "  m2=[]\n",
        "  m3=[]\n",
        "  m4=[]\n",
        "  p1=[]\n",
        "  p2=[]\n",
        "  p3=[]\n",
        "  p4=[]\n",
        "  ErrFent=[]\n",
        "  ErrLent=[]\n",
        "  ErrBal=[]\n",
        "  ErrJobb=[]\n",
        "  img1=np.copy(img)\n",
        "  img2=np.copy(img)\n",
        "  img3=np.copy(img)\n",
        "  img4=np.copy(img)\n",
        "  imgGray=np.copy(img)\n",
        "  imgGray=cv2.cvtColor(imgGray,cv2.COLOR_RGB2GRAY)\n",
        "  imgcr=np.copy(img)\n",
        "  imgcrit1=np.copy(img)\n",
        "  imgcrit2=np.copy(img)\n",
        "  dilation1=0\n",
        "  dilation2=0\n",
        "  Fekete=[]\n",
        "  c=0\n",
        "  d=0\n",
        "  while(c<480):\n",
        "    sor=[]\n",
        "    d=0\n",
        "    while(d<640):\n",
        "      sor.append(0)\n",
        "      d+=1\n",
        "    Fekete.append(sor)\n",
        "    c+=1\n",
        "  Fekete=np.copy(img)\n",
        "  for i in obje:\n",
        "    ErrFent.append(concentrate(i[1]-i[3]))\n",
        "    ErrBal.append(concentrate(i[0]-i[2]))\n",
        "    ErrLent.append(concentrate(i[1]+i[3]))\n",
        "    ErrJobb.append(concentrate(i[0]+i[2]))\n",
        "  mask1 = cv2.inRange(imgGray, np.array([0]), np.array([100]))\n",
        "  imgcrit1=np.copy(img1)\n",
        "  imgcrit1[mask1 != [255]]=[0,0,0]\n",
        "  imgcrit1=cv2.morphologyEx(imgcrit1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)))\n",
        "  imgs_canny=cv2.Canny(mask1,50,120)\n",
        "  dilation1=cv2.dilate(imgs_canny,np.ones((5,5),np.uint8),iterations=1)\n",
        "  dilation1=cv2.morphologyEx(dilation1, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10)))\n",
        "  dilation1=cv2.morphologyEx(dilation1, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10)))\n",
        "  contours=[]\n",
        "  (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  for pic, contour in enumerate(contours):\n",
        "    area=cv2.contourArea(contour)\n",
        "    redParam=area\n",
        "    if(800<area and area<5000):\n",
        "      peri=cv2.arcLength(contour,True)\n",
        "      approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      bol=True\n",
        "      cntr=0\n",
        "      if(bol and y>50):\n",
        "        p1.append(x)\n",
        "        p2.append(y)\n",
        "        p3.append(x+w)\n",
        "        p4.append(y+h)\n",
        "        cv2.rectangle(img2, (x , y ), (x + w , y + h ), (255, 255, 255), -1)\n",
        "        cv2.putText(img2,(str(area)), (x+5,y+15), cv2.FONT_HERSHEY_COMPLEX_SMALL,1.5,(255, 255, 255), 2)\n",
        "  img2=cv2.inRange(img2, np.array([250,250,250]), np.array([255,255,255]))\n",
        "  img3[img2 != [255]]=[0,0,0]\n",
        "  imgGray=np.copy(img3)\n",
        "  imgGray=cv2.cvtColor(imgGray,cv2.COLOR_RGB2GRAY)\n",
        "  imgGray = cv2.inRange(imgGray, np.array([0]), np.array([60]))\n",
        "  imgGray=cv2.morphologyEx(imgGray, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(7,7)))\n",
        "  imgs_canny=cv2.Canny(imgGray,50,120)\n",
        "  dilation1=cv2.dilate(imgs_canny,np.ones((3,3),np.uint8),iterations=1)\n",
        "  contours=[]\n",
        "  (contours,hierarchy)=cv2.findContours(dilation1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  for pic, contour in enumerate(contours):\n",
        "    area=cv2.contourArea(contour)\n",
        "    if(area>3000 and area<20000):\n",
        "      peri=cv2.arcLength(contour,True)\n",
        "      approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      cv2.rectangle(imgcr, (x , y ), (x + w , y + h ), (255, 255, 255), -1)\n",
        "      cv2.rectangle(Fekete, (x , y ), (x + w , y + h ), (255,255,255), -1)\n",
        "  imgcr=cv2.inRange(imgcr, np.array([250,250,250]), np.array([255,255,255]))\n",
        "  Fekete=cv2.inRange(Fekete, np.array([250,250,250]), np.array([255,255,255]))\n",
        "  imgGray=np.copy(img4)\n",
        "  imgGray[imgcr != [255]]=[0,0,0]\n",
        "  imgGray=cv2.cvtColor(imgGray,cv2.COLOR_RGB2GRAY)\n",
        "  img_eq = cv2.equalizeHist(imgGray)\n",
        "  hist2 = cv2.calcHist([img_eq], [0], None, [256], [0,256])\n",
        "  Ranger=cv2.inRange(img_eq, np.array([0]), np.array([120]))\n",
        "  Ranger[Fekete == 0]=0\n",
        "  detected_circles = cv2.HoughCircles(Ranger,cv2.HOUGH_GRADIENT, 1, 25, param1 = 40, param2 = 20, minRadius = 9, maxRadius = 20) #40-20 egész jó volt\n",
        "  if detected_circles is not None:     \n",
        "      detected_circles = np.uint16(np.around(detected_circles))     \n",
        "      for pt in detected_circles[0, :]:\n",
        "          a, b, r = pt[0], pt[1], pt[2] \n",
        "          img=cv2.circle(img, (a, b), r, (128,255,255), -1)\n",
        "          m1.append(a)\n",
        "          m2.append(b)\n",
        "          m3.append(2*r)\n",
        "          m4.append(2*r)\n",
        "\n",
        "  Ranger2=np.copy(Ranger)\n",
        "  (contours,hierarchy) = cv2.findContours(Ranger, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  coordinates = []\n",
        "  for cnt in contours:\n",
        "    area=cv2.contourArea(contour)\n",
        "    approx = cv2.approxPolyDP(cnt, 0.07 * cv2.arcLength(cnt, True), True)\n",
        "    x , y , w, h = cv2.boundingRect(approx)\n",
        "    if (len(approx) == 3 and (w*h)<890 and (w*h)>700):\n",
        "        #cv2.drawContours(img, [cnt], 0, (128, 255, 255), -1) #Itt\n",
        "        #cv2.putText(img,(str(w*h)), (x+5,y+15), cv2.FONT_HERSHEY_COMPLEX_SMALL,1.5,(0, 0, 0), 2)\n",
        "        m1.append(x+w//2)\n",
        "        m2.append(y+h//2)\n",
        "        m3.append(w)\n",
        "        m4.append(h)\n",
        "  (contours,hierarchy) = cv2.findContours(Ranger, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  coordinates = []\n",
        "  for cnt in contours:\n",
        "    area=cv2.contourArea(contour)\n",
        "    if(area>5000):\n",
        "      peri=cv2.arcLength(contour,True)\n",
        "      approx = cv2.approxPolyDP(contour, 0.07 * peri, True)\n",
        "      x , y , w, h = cv2.boundingRect(approx)\n",
        "      cv2.rectangle(Ranger2, (x , y ), (x + w , y + h ), (0,0,0), -1)\n",
        "  return bol, img, big, m1,m2,m3,m4,Ranger2,Ranger,Ranger2,blackParam\n",
        "\n",
        "def kozl(myImg):\n",
        "  bol=False\n",
        "  m1=[]\n",
        "  m2=[]\n",
        "  m3=[]\n",
        "  m4=[]\n",
        "  i=np.copy(myImg)\n",
        "  img=np.copy(i)\n",
        "  img2=img.reshape((-1,3))\n",
        "  img2=np.float32(img2)\n",
        "  criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,10,1.0)\n",
        "  k=3\n",
        "  attempts=10\n",
        "  ret,label,center=cv2.kmeans(img2,k,None,criteria,attempts,cv2.KMEANS_PP_CENTERS)\n",
        "  center=np.uint8(center)\n",
        "  res=center[label.flatten()]\n",
        "  res2=res.reshape((img.shape))\n",
        "  gray=cv2.cvtColor(res2,cv2.COLOR_RGB2GRAY)\n",
        "  bined=cv2.inRange(gray,np.array([0]),np.array([128]))\n",
        "  contours,_ = cv2.findContours(bined, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "  for cnt in contours:\n",
        "    area=cv2.contourArea(cnt)\n",
        "    peri=cv2.arcLength(cnt,True)\n",
        "    approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)\n",
        "    x , y , w, h = cv2.boundingRect(approx)\n",
        "    if(area>200.0 and area<3000.0 and (x+w//2)<620 and (y+h//2)<420 and y>20 and w<np.int(1.3*h) and h<np.int(1.3*w)):\n",
        "      bol=True\n",
        "      m1.append(x+w//2)\n",
        "      m2.append(y+h//2)\n",
        "      m3.append(w)\n",
        "      m4.append(h)\n",
        "      cv2.drawContours(img, cnt, -1, (0,255,255),-1)\n",
        "      #cv2.putText(img,str(area),(x+5,y+10),cv2.FONT_HERSHEY_COMPLEX_SMALL,1.25,(255,0,0))\n",
        "  return bol, img, m1,m2,m3,m4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOTJfoULdtTy",
        "colab_type": "text"
      },
      "source": [
        "# Andris + Vektor Work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EXqOnb0b-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yaml\n",
        "import math\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(30,300))\n",
        "from PIL import Image\n",
        "# Read first frame\n",
        "myAnswers={} #Evaluationhoz\n",
        "myPred={}\n",
        "myObjects=[]\n",
        "u=[]  #Feltöltéshez\n",
        "v=[]\n",
        "w=[]\n",
        "h=[]\n",
        "imgsRgb=[] #Számításokhoz\n",
        "imgsRong=[] #Rongáláshoz\n",
        "imgSeg=[]\n",
        "kernel_dil=np.ones((20,20),np.uint8)\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
        "\n",
        "cntr=0\n",
        "for i in allNames:\n",
        "  imgsRgb.append(cv2.imread(allDestination[cntr]))\n",
        "  imgsRgb[cntr]=cv2.cvtColor(imgsRgb[cntr],cv2.COLOR_BGR2RGB)\n",
        "  imgsRong.append(np.copy(imgsRgb[cntr]))\n",
        "  cntr+=1\n",
        "\n",
        "\n",
        "#Arculás\n",
        "faces=[]\n",
        "faced=[]\n",
        "facedx=[]\n",
        "faces.append(cv2.imread('Faces/0.PNG'))\n",
        "faces.append(cv2.imread('Faces/1.PNG'))\n",
        "faces.append(cv2.imread('Faces/2.PNG'))\n",
        "faces.append(cv2.imread('Faces/3.PNG'))\n",
        "faced.append(cv2.imread('Faces/Faced/0.png'))\n",
        "faced.append(cv2.imread('Faces/Faced/1.png'))\n",
        "faced.append(cv2.imread('Faces/Faced/2.png'))\n",
        "faced.append(cv2.imread('Faces/Faced/3.png'))\n",
        "facedx.append(cv2.imread('Faces/Faced/H2.png'))\n",
        "facedx.append(cv2.imread('Faces/Faced/S2.png'))\n",
        "faced[0]=cv2.cvtColor(faced[0], cv2.COLOR_BGR2GRAY)\n",
        "faced[1]=cv2.cvtColor(faced[1], cv2.COLOR_BGR2GRAY)\n",
        "faced[2]=cv2.cvtColor(faced[2], cv2.COLOR_BGR2GRAY)\n",
        "faced[3]=cv2.cvtColor(faced[3], cv2.COLOR_BGR2GRAY)\n",
        "facedx[0]=cv2.cvtColor(facedx[0], cv2.COLOR_BGR2GRAY)\n",
        "facedx[1]=cv2.cvtColor(facedx[1], cv2.COLOR_BGR2GRAY)\n",
        "#Detect Faces\n",
        "cntr=0\n",
        "cntr2=0\n",
        "invFaces=[]\n",
        "for i in faces:\n",
        "  print('asd')\n",
        "  img=np.copy(faces[cntr])\n",
        "  gray=np.copy(faces[cntr])\n",
        "  gray= cv2.cvtColor(gray, cv2.COLOR_RGB2GRAY)\n",
        "  gray = cv2.blur(gray, (3, 3))\n",
        "  #plt.imshow(gray)\n",
        "  detected_circles = cv2.HoughCircles(gray,cv2.HOUGH_GRADIENT, 1, 15, param1 = 80, param2 = 40, minRadius = 7, maxRadius = 40) #70-35 egész jó volt\n",
        "  if detected_circles is not None: \n",
        "    # Convert the circle parameters a, b and r to integers. \n",
        "    detected_circles = np.uint16(np.around(detected_circles)) \n",
        "    for pt in detected_circles[0, :]:\n",
        "        mask=np.copy(img)\n",
        "        maskhelp=np.copy(img) \n",
        "        a, b, r = pt[0], pt[1], pt[2] \n",
        "        mask=cv2.circle(mask, (a, b), r, (255,255,255), -1)\n",
        "        mask=cv2.inRange(mask,np.array([254,254,254]), np.array([255,255,255]))\n",
        "        maskhelp[mask != [255]]=[0,0,0]\n",
        "        faces[cntr]=(maskhelp[(b-r):(b+r),(a-r):(a+r)])\n",
        "        faces[cntr]=cv2.cvtColor(faces[cntr], cv2.COLOR_RGB2GRAY)\n",
        "        faces[cntr] = cv2.morphologyEx(faces[cntr], cv2.MORPH_OPEN, np.ones((2,2)))\n",
        "        faces[cntr] = cv2.equalizeHist(faces[cntr])\n",
        "        faces[cntr]=cv2.inRange(faces[cntr],np.array([50]), np.array([255]))\n",
        "        length=min(faces[cntr].shape[1],faces[cntr].shape[0])\n",
        "        faces[cntr] = cv2.resize(faces[cntr], (length,length), interpolation = cv2.INTER_AREA)\n",
        "        invFaces.append(np.copy(faces[cntr]))\n",
        "        invFaces[cntr][invFaces[cntr]!=0]=0\n",
        "        invMask=np.copy(invFaces[cntr])\n",
        "        invMask=cv2.circle(invMask, (r, r), r-10, (255), -1)\n",
        "        invFaces[cntr][faces[cntr]==0]=255\n",
        "        invFaces[cntr][invMask==0]=0\n",
        "        invFaces[cntr]=cv2.circle(invFaces[cntr], (r, r), r, (0), 5)\n",
        "        cntr2+=1    \n",
        "  cntr+=1\n",
        "\n",
        "\n",
        "kernel = np.ones((8,8), np.uint8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Ezt Használjuk Igazán\n",
        "cntr=0\n",
        "cntr2=0\n",
        "cntr3=0\n",
        "print('bármi kerestetik')\n",
        "for i in allNames:\n",
        "  myPred={}\n",
        "  myObjects=[]  \n",
        "  img=np.copy(imgsRong[cntr])\n",
        "  gray=np.copy(img)\n",
        "  gray= cv2.cvtColor(gray, cv2.COLOR_RGB2GRAY)\n",
        "  gray = cv2.blur(gray, (3, 3))\n",
        "  detected_circles = cv2.HoughCircles(gray,cv2.HOUGH_GRADIENT, 1, 25, param1 = 90, param2 = 45, minRadius = 7, maxRadius = 50) #90-45 egész jó volt\n",
        "  c0=[]\n",
        "  c1=[]\n",
        "  c2=[]\n",
        "  c3=[]\n",
        "  if detected_circles is not None:     \n",
        "      detected_circles = np.uint16(np.around(detected_circles))     \n",
        "      for pt in detected_circles[0, :]:\n",
        "          a, b, r = pt[0], pt[1], pt[2] \n",
        "          Croped=np.copy(img)\n",
        "          p1=b-r\n",
        "          p2=b+r\n",
        "          p3=a-r\n",
        "          p4=a+r\n",
        "          if(p1>1000):\n",
        "            p1=0\n",
        "          if(p3>1000):\n",
        "            p3=0\n",
        "          Crop=Croped[(p1):(p2),(p3):(p4)]\n",
        "          Crop=cv2.cvtColor(Crop, cv2.COLOR_RGB2GRAY)\n",
        "          Crop=cv2.inRange(Crop,np.array([80]), np.array([255]))\n",
        "          essence=(np.int(cv2.countNonZero(Crop))>np.int(0.9*(r*r*3)))\n",
        "          \n",
        "          bol,imghelp,big,m1,m2,m3,m4,i1,i2,i3,redParam=isReallyCactus(a,b,r,img)\n",
        "          if (essence==True and m1!=0 and m2!=0 and m3!=0 and m4!=0):\n",
        "\n",
        "\n",
        "            #Osztályozzuk\n",
        "            mask=np.copy(imgsRong[cntr])\n",
        "            maskhelp=np.copy(imgsRong[cntr]) \n",
        "            mask=cv2.circle(mask, (a, b), r, (255,255,255), -1)\n",
        "            mask=cv2.inRange(mask,np.array([250,250,250]), np.array([255,255,255]))\n",
        "            maskhelp[mask != [255]]=[0,0,0]\n",
        "            maskhelp=cv2.Canny(maskhelp,50,120)\n",
        "            \"\"\"\n",
        "            #KMeans\n",
        "            img11=np.copy(imgsRgb[cntr])\n",
        "            img22=img11.reshape((-1,3))\n",
        "            img22=np.float32(img22)\n",
        "            criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,10,1.0)\n",
        "            k=2\n",
        "            attempts=10\n",
        "            ret,label,center=cv2.kmeans(img22,k,None,criteria,attempts,cv2.KMEANS_PP_CENTERS)\n",
        "            center=np.uint8(center)\n",
        "            res=center[label.flatten()]\n",
        "            res2=res.reshape((img11.shape))\n",
        "            gray=cv2.cvtColor(res2,cv2.COLOR_RGB2GRAY)\n",
        "            bined=cv2.inRange(gray,np.array([0]),np.array([128]))\n",
        "            maskhelp=np.copy(bined)\n",
        "            \"\"\"\n",
        "            imgsCrop=maskhelp[(b-r):(b+r),(a-r):(a+r)]\n",
        "            #imgsCrop=cv2.cvtColor(imgsCrop, cv2.COLOR_RGB2GRAY)\n",
        "            if(True):\n",
        "              ter=1000\n",
        "              cntr3=0\n",
        "              num=0\n",
        "              for j in facedx:\n",
        "                print(\"Mosoly::\",cv2.countNonZero(np.copy(facedx[0])))\n",
        "                print(\"Szomoly::\",cv2.countNonZero(np.copy(facedx[1])))\n",
        "                simg=np.copy(imgsCrop)\n",
        "                #simg = cv2.morphologyEx(simg, cv2.MORPH_OPEN, np.ones((2,2)))\n",
        "                #simg = cv2.equalizeHist(simg)\n",
        "                #simg=cv2.inRange((simg),np.array([128]), np.array([255]))\n",
        "                invCopy=np.copy(invFaces[cntr3])\n",
        "                invCopy = cv2.dilate(invCopy, kernel, iterations=1)\n",
        "                w1=simg.shape[1]\n",
        "                h1=simg.shape[0]\n",
        "                dimension = (w1, h1) \n",
        "                helper=np.copy(j)\n",
        "                helper= cv2.resize(helper, dimension, interpolation = cv2.INTER_AREA) #Ez új méretező\n",
        "                eTer=cv2.countNonZero(helper)\n",
        "                #w1=j.shape[1]\n",
        "                #h1=j.shape[0]\n",
        "                #dimension = (w1, h1) \n",
        "                #simg = cv2.resize(simg, dimension, interpolation = cv2.INTER_AREA)\n",
        "                #Szem terület arány:\n",
        "                sized=np.copy(simg)\n",
        "                o1=np.int(w1*0.25)\n",
        "                o2=np.int(h1*0.13)\n",
        "                o3=np.int(w1*0.75)\n",
        "                o4=np.int(h1*0.43)\n",
        "                #print(\"o1:\",o1)\n",
        "                #print(\"o2:\",o2)\n",
        "                #print(\"o3:\",o3)\n",
        "                #print(\"o4:\",o4)\n",
        "                sized=sized[o2:o4,o1:o3]\n",
        "                xRea=w1*h1\n",
        "                xRea=xRea*0.5\n",
        "                xRea=xRea*0.3\n",
        "                szem=(cv2.countNonZero(sized)/xRea)\n",
        "                #print(\"xRea:\",xRea)\n",
        "                #print(\"sized:\",sized)\n",
        "                #Idáig\n",
        "                solution=simg-helper\n",
        "                solution=cv2.circle(solution, (w1//2, h1//2), min(w1,h1)//2, (0), 8)\n",
        "                solution2=np.copy(simg)\n",
        "                mask=np.zeros((h1, w1))\n",
        "                cv2.rectangle(mask, (0,0), (w1,h1//2), (255), -1)\n",
        "                mask[simg==255]=0\n",
        "                cv2.rectangle(mask, (0,h1//2), (w1,h1), (0), -1)\n",
        "                solution2[simg==255]=0\n",
        "                solution2[simg==0]=255\n",
        "                #facedx[cntr3] = cv2.resize(facedx[cntr3], dimension, interpolation = cv2.INTER_AREA)\n",
        "                helper[solution2==255]=0\n",
        "                cv2.rectangle(solution2,(np.int(w1*0.25),np.int(h1*0.13)),(np.int(w1*0.75),np.int(h1*0.43)),0,-1)\n",
        "                back=cv2.countNonZero(helper)\n",
        "                area=cv2.countNonZero(helper)/cv2.countNonZero(j)\n",
        "                veszt=eTer-cv2.countNonZero(helper)\n",
        "                if(num==1):\n",
        "                  veszt=veszt*1.05\n",
        "                asd=np.copy(facedx[cntr3])\n",
        "                print('Ter:', ter)\n",
        "                print('Veszt:', veszt)\n",
        "                if(veszt<ter):\n",
        "                  print(\"Got in\")\n",
        "                  print(\"with:\",veszt)\n",
        "                  ter=veszt\n",
        "                  num=cntr3\n",
        "                cv2.putText(helper, str(szem), (5, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                cv2.putText(helper, str(veszt), (5, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                #cv2.putText(helper, str(back), (5, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                #cv2.putText(helper, str(cv2.countNonZero(simg)), (30, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                #cv2.putText(solution2, str((cv2.countNonZero(solution2))), (5, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (0))\n",
        "                #cv2.putText(asd, str((cv2.countNonZero(facedx[cntr3]))), (5, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                #cv2.putText(mask, str((cv2.countNonZero(mask))-800), (5, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (0))\n",
        "                #cv2.putText(simg, str((cv2.countNonZero(simg))), (5, 10), cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5, (255))\n",
        "                cntr3+=1\n",
        "                if(False):\n",
        "                  cntr2+=1\n",
        "                  plt.subplot(60,3,cntr2*3+1)\n",
        "                  plt.imshow(simg,cmap='gray')\n",
        "                  plt.subplot(60,3,cntr2*3+2)\n",
        "                  plt.imshow(helper,cmap='gray')\n",
        "                  plt.subplot(60,3,cntr2*3+3)\n",
        "                  plt.imshow(j,cmap='gray')\n",
        "                if(num==1):\n",
        "                  if(szem>0.2):\n",
        "                    num=2\n",
        "                if(num==0):\n",
        "                  num=0\n",
        "                  if(szem>0.2):\n",
        "                    num=3      \n",
        "            print(\"Siker: \",cv2.countNonZero(Crop), np.int(0.8*(r*r*3)), essence)\n",
        "            myObjects.append([m1,m2,m3,m4,2,num,0,0,0])\n",
        "          elif(a!=0 and b!=0 and r!=0):\n",
        "            print(m1,m2,m3,m4)\n",
        "            print(\"Fail: \",cv2.countNonZero(Crop), np.int(0.8*(r*r*3)), essence)\n",
        "            myObjects.append([a,b,r*2,r*2,0,0,0,0,0])\n",
        "            img=cv2.rectangle(img, (a-r , b-r ), (a + r , b + r ), (255, 255, 255), -1)\n",
        "\n",
        "  #Tablak\n",
        "  bol,imghelp,p0,p1,p2,p3=kozl(img)\n",
        "  img=imghelp\n",
        "  if (bol):\n",
        "    cnt=0\n",
        "    for i in p0:\n",
        "      myObjects.append([p0[cnt],p1[cnt],p2[cnt],p3[cnt],0,0,0,0,0])\n",
        "      cnt+=1\n",
        "  #Truck\n",
        "  bol,imghelp,big,p0,p1,p2,p3,i1,i2,i3,redParam=isTruck(1,1,img)\n",
        "  img=imghelp\n",
        "  if (bol):\n",
        "    myObjects.append([p0,p1,p2,p3,1,1,0,0,0])\n",
        "  #ITS A PLANEEEEE AVAGY A SOKADIK HAJNALI PROGRAMOZÁSNÁL....\n",
        "  bol,imghelp,big,p0,p1,p2,p3,i1,i2,i3,redParam=isRepzi(1,2,img)\n",
        "  img=imghelp\n",
        "  if (bol):\n",
        "    myObjects.append([p0,p1,p2,p3,1,2,0,0,0])\n",
        "  #SUV\n",
        "  bol,imghelp,big,p0,p1,p2,p3,i1,i2,i3,redParam=isSUV(1,0,img)\n",
        "  img=imghelp\n",
        "  if (bol and p1>200):\n",
        "    myObjects.append([p0,p1,p2,p3,1,0,0,0,0])\n",
        "  \"\"\"\n",
        "  #More Tables\n",
        "  yourObject=myObjects.copy()\n",
        "  bol,imghelp,big,p0,p1,p2,p3,i1,i2,i3,redParam=redTable(img,yourObject)\n",
        "  c=0\n",
        "  for j in p0:\n",
        "    myObjects.append([p0[c],p1[c],p2[c],p3[c],0,0,0,0,0])\n",
        "    c+=1\n",
        "  img=imghelp\n",
        "  imgSeg.append(np.copy(imghelp))\n",
        "  \"\"\"\n",
        "  myPred[\"objects\"] = myObjects\n",
        "  myPred[\"poses\"] = [0,0,0,0,0,0,0,0,0,0,0,0] \n",
        "  print(\"allDestination[cntr]\",myObjects)\n",
        "  myAnswers[allDestination[cntr]]=myPred\n",
        "\n",
        "  #plt.subplot(24,3,cntr+1)\n",
        "  cntr2+=1\n",
        "  #plt.imshow(drawBBs(myAnswers[allDestination[cntr]][\"objects\"],imgsRgb[cntr]))\n",
        "  cntr+=1\n",
        "\n",
        "\n",
        "with open(\"HW/calibration.yaml\") as file:\n",
        "  file.readline()\n",
        "  file.readline()\n",
        "  documents = yaml.load(file)\n",
        "  for item,data in documents.items():\n",
        "    print(item, ':', data)\n",
        "    if item == 'image_height':\n",
        "      image_height = data\n",
        "    elif item == 'image_width':\n",
        "      image_width = data\n",
        "    elif item == 'camera_matrix':\n",
        "      for item_2,data_2 in data.items():\n",
        "        if item_2 == 'data':\n",
        "          camera_matrix = data_2\n",
        "    elif item == 'distortion_coefficients':\n",
        "      for item_2,data_2 in data.items():\n",
        "        if item_2 == 'data':\n",
        "          distortion_coefficients = data_2\n",
        "fx = camera_matrix[0]\n",
        "px = camera_matrix[2]\n",
        "fy = camera_matrix[4]\n",
        "py = camera_matrix[5]\n",
        "camera_matrix = [camera_matrix[0:3], camera_matrix[3:6], camera_matrix[6:9]]\n",
        "camera_matrix = np.array(camera_matrix)\n",
        "distortion_coefficients = np.array(distortion_coefficients)\n",
        "\n",
        "for item,data in myAnswers.items():\n",
        "  item = item[:-3]\n",
        "  first,second = item.split(\"rgb\")\n",
        "  item = first + 'depth' + second + 'png'\n",
        "  img = cv2.imread(item,-1)\n",
        "  img_2 = cv2.undistort(img, camera_matrix, distortion_coefficients, None, camera_matrix)\n",
        "  map_u, map_v = cv2.initUndistortRectifyMap(camera_matrix, distortion_coefficients, None, camera_matrix, (image_width,image_height), cv2.CV_32FC1)\n",
        "  for item_2,data_2 in data.items():\n",
        "    if item_2 == \"objects\":\n",
        "      for myobject in data_2:\n",
        "        u = myobject[0]\n",
        "        v = myobject[1]\n",
        "        u_new = map_u[v,u]\n",
        "        v_new = map_v[v,u]\n",
        "        print(int(v_new),int(u_new))\n",
        "        #solution where depth = z\n",
        "        z = img[int(v_new),int(u_new)]\n",
        "        z = z\n",
        "        x = (z*u_new-z*px)/fx\n",
        "        y = (z*v_new-z*py)/fy\n",
        "        y = -y\n",
        "        #solution where depth = sqrt(z^2+x^2+y^2)\n",
        "        '''\n",
        "        d = img[int(v_new),int(u_new)]\n",
        "        a = fy**2+(v**2-py**2)-((u**2-px**2)*(v**2-py**2))/(u**2+fx**2-px**2)\n",
        "        b = d**2*((v**2-py**2)-((u**2-px**2)*(v**2-py**2))/(u**2+fx**2-px**2))\n",
        "        print(a)\n",
        "        print(b)\n",
        "        y = b/a\n",
        "        if y < 0:\n",
        "          y = -math.sqrt(-y)\n",
        "        else:\n",
        "          y = math.sqrt(y)\n",
        "        x = (-((y**2-d**2)*(u**2-px**2))/(u**2+fx**2-px**2))\n",
        "        if x < 0:\n",
        "          x = -math.sqrt(-x)\n",
        "        else:\n",
        "          x = math.sqrt(x)\n",
        "        z = math.sqrt(d**2-x**2-y**2)\n",
        "        '''\n",
        "        z = z/1000\n",
        "        y = y/1000\n",
        "        x = x/1000\n",
        "        print(x, y, z)\n",
        "        myobject[6] = x\n",
        "        myobject[7] = y\n",
        "        myobject[8] = z\n",
        "print(fx)\n",
        "print(fy)\n",
        "print(px)\n",
        "print(py)\n",
        "\n",
        "\"\"\"\n",
        "cntr=0\n",
        "for i in imgsRgb:\n",
        "  plt.subplot(24,3,cntr+1)\n",
        "  plt.imshow(drawBBs(myAnswers[allDestination[cntr]][\"objects\"], i))\n",
        "  cntr+=1\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pdPfUGIKcZ2",
        "colab_type": "text"
      },
      "source": [
        "#Barni\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAeQS4XkKfPp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8079346-a1c4-4cf8-a051-0c3648e52ca9"
      },
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "'''plt.figure(figsize=(30,30))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(img1)\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(img2)'''\n",
        "\n",
        "'''\n",
        "A = np.eye(4)\n",
        "print(A)\n",
        "\n",
        "objectsPrev = []\n",
        "myMatrix = []\n",
        "src = []\n",
        "dst = []\n",
        "#print(np.array(myAnswers))\n",
        "#x = myAnswers.items()\n",
        "#print('with item():',x)\n",
        "objparamPrev = []\n",
        "for imgname, objects in myAnswers.items():\n",
        "  print ('\\n',imgname, ' : ')\n",
        "\n",
        "  for objects2 in objects.items():\n",
        "    if objects2[0] == 'objects':\n",
        "      objparam = np.array(objects2[1][:][:])\n",
        "      if(objparamPrev == []):\n",
        "        print('first')\n",
        "        objparamPrev = objparam\n",
        "        A = np.eye(4)\n",
        "        myMatrix.append(A)\n",
        "        print(np.array(myMatrix).reshape(-1,4,4))\n",
        "      else:\n",
        "        objparam = np.array(objects2[1][:][:])\n",
        "        #print(objparam[:])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #give src the x,y,z params\n",
        "        for row in objparam:\n",
        "          #print('first row:', row)\n",
        "          print('\\ncoords of object:')\n",
        "          print(row[6:9])\n",
        "          for row2 in objparamPrev:\n",
        "            #print('second row:', row2)\n",
        "            distu = row[0]-row2[0]\n",
        "            distv = row[1]-row2[1]\n",
        "            \n",
        "            #print('distu, distv:', distu, distv)\n",
        "            if abs(distu) < 100 and abs(distv)<100:\n",
        "              if row[4] == row2[4]:\n",
        "                print('possible match:')\n",
        "                print(row2[6:9])\n",
        "            \n",
        "\n",
        "              #src.append((objparamPrev[match][6], objparam[match][7], objparam[match][8]))\n",
        "              #dst.append((objparam[match][6], objparam[match][7], objparam[match][8]))\n",
        "        objparamPrev = objparam\n",
        "\n",
        "\n",
        "\n",
        "src = np.array([\n",
        "    [10, 10, 10],\n",
        "    [10, 10, 20],\n",
        "    [10, 20, 10],\n",
        "    [10, 20, 20],\n",
        "])\n",
        "\n",
        "dst = np.array([\n",
        "    [10, 10, 20],\n",
        "    [10, 10, 30],\n",
        "    [10, 20, 20],\n",
        "    [10, 20, 30],\n",
        "])\n",
        "print(dst)\n",
        "\n",
        "retval, out, inliners = cv2.estimateAffine3D(src, dst, ransacThreshold = 0.1)\n",
        "\n",
        "#print(names1)\n",
        "#print(retval,'\\nA matrice:\\n', out,'\\ninliners:', inliners)\n",
        "\n",
        "#kiszámolni a közös pontokat [u, v] koordináta-> u1, v1, u2, v2\n",
        "#depth képből kiszámolni az [x,y,z] koordinátát az adott pontokhoz -> [ui, vi] ->[xi, yi, zi]\n",
        "#cv2.estimateAffine3D(src, dst[, out[, inliers[, ransacThreshold[, confidence]]]]) → retval, out, inliers ->legjobb A közelítés\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################## IT ALL STARTED HERE ###################################\n",
        "\n",
        "outPrev = np.eye(4)\n",
        "imgprevu = []\n",
        "map_u, map_v = cv2.initUndistortRectifyMap(camera_matrix, distortion_coefficients, None, camera_matrix, (image_width,image_height), cv2.CV_32FC1)\n",
        "poses = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#read images\n",
        "'''\n",
        "imgprev = cv2.imread('HW/g1/rgb/1.jpg')\n",
        "imgprevdpt = cv2.imread('HW/g1/depth/1.png', -1)\n",
        "imgprev = cv2.cvtColor(imgprev, cv2.COLOR_BGR2GRAY)\n",
        "imgprevu = cv2.undistort(imgprev, camera_matrix, distortion_coefficients, None, camera_matrix)\n",
        "imgprevdptu = cv2.undistort(imgprevdpt, camera_matrix, distortion_coefficients, None, camera_matrix)'''\n",
        "\n",
        "for item, data in myAnswers.items():\n",
        "  imgpath_rgb = item\n",
        "  print(imgpath_rgb)\n",
        "  item = item[:-3]\n",
        "  first,second = item.split(\"rgb\")\n",
        "  imgpath_dpt = first + 'depth' + second + 'png'\n",
        "\n",
        "  img = cv2.imread(imgpath_rgb)\n",
        "  imgdpt = cv2.imread(imgpath_dpt, -1)\n",
        "\n",
        "  #getting gray images\n",
        "\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  #undistorting img\n",
        "\n",
        "  imgu = cv2.undistort(img, camera_matrix, distortion_coefficients, None, camera_matrix)\n",
        "  imgdptu = cv2.undistort(imgdpt, camera_matrix, distortion_coefficients, None, camera_matrix)\n",
        "\n",
        "  #első elem vizsgálata:\n",
        "  if imgprevu == []:\n",
        "\n",
        "    out = np.eye(4)\n",
        "    poses.append(out)\n",
        "\n",
        "    outPrev = out\n",
        "    imgprevu = imgu\n",
        "    imgprevdptu = imgdptu\n",
        "    print(out)\n",
        "\n",
        "  else:\n",
        "    #keypoint matching\n",
        "    orb = cv2.ORB_create()\n",
        "    kpprev, desprev = orb.detectAndCompute(imgprevu, None)\n",
        "    kp, des = orb.detectAndCompute(imgu, None)\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
        "    matches = bf.knnMatch(desprev, des, k = 2)\n",
        "\n",
        "\n",
        "    list_kpprev = []\n",
        "    list_kp = []\n",
        "\n",
        "    #getting the good match coordinates\n",
        "    good = []\n",
        "    for m, n in matches:\n",
        "      if m.distance < 0.55*n.distance:\n",
        "        good.append([m])\n",
        "\n",
        "        imgprev_idx = m.queryIdx\n",
        "        img_idx = m.trainIdx\n",
        "        \n",
        "        (x1, y1) = kpprev[imgprev_idx].pt\n",
        "        (x2, y2) = kp[img_idx].pt\n",
        "        list_kpprev.append((x1, y1))\n",
        "        list_kp.append((x2, y2))\n",
        "\n",
        "    array_kpprev = np.uint16(list_kpprev).reshape(-1,1,2)\n",
        "    array_kp = np.uint16(list_kp).reshape(-1,1,2)\n",
        "\n",
        "\n",
        "  #checking if it is a new series of frame\n",
        "    if len(list_kp) < 10:\n",
        "      print('new frameset :', len(list_kp))\n",
        "\n",
        "      out = np.eye(4)\n",
        "      print(out)\n",
        "      poses.append(out)\n",
        "      outPrev = out\n",
        "      imgprevu = imgu\n",
        "      imgprevdptu = imgdptu\n",
        "\n",
        "\n",
        "    else:\n",
        "      print('matches :', len(list_kp))\n",
        "      matpointsprev = []\n",
        "      matpoints = []\n",
        "\n",
        "      #getting xyz coordinates for the prev image\n",
        "      for i in list_kpprev:\n",
        "\n",
        "          u = int(i[0])\n",
        "          v = int(i[1])\n",
        "          \n",
        "          u_new = map_u[v,u]\n",
        "          v_new = map_v[v,u]\n",
        "\n",
        "          #solution where depth = z\n",
        "          z = imgprevdptu[int(v_new),int(u_new)]\n",
        "          z = z/1000\n",
        "          #print(z)\n",
        "          x = (z*u_new-z*px)/fx\n",
        "          y = (z*v_new-z*py)/fy\n",
        "          y = -y\n",
        "          matpointsprev.append([x,y,z])\n",
        "          print\n",
        "\n",
        "      #getting xyz coordinates for the actual image\n",
        "      for i in list_kp:\n",
        "\n",
        "          u = int(i[0])\n",
        "          v = int(i[1])\n",
        "          \n",
        "          u_new = map_u[v,u]\n",
        "          v_new = map_v[v,u]\n",
        "          \n",
        "\n",
        "          #solution where depth = z\n",
        "          z = imgdptu[int(v_new),int(u_new)]\n",
        "          z = z/1000\n",
        "          \n",
        "          x = (z*u_new-z*px)/fx\n",
        "          y = (z*v_new-z*py)/fy\n",
        "          y = -y\n",
        "          matpoints.append([x,y,z])\n",
        "\n",
        "      matpointsprev = np.array(matpointsprev).reshape(-1,3) \n",
        "      matpoints = np.array(matpoints).reshape(-1,3)   \n",
        "      #print(matpoints)\n",
        "      #print(matpointsprev)\n",
        "\n",
        "      #getting affine matrix\n",
        "\n",
        "      retval, out, liners = cv2.estimateAffine3D(matpointsprev, matpoints, ransacThreshold=0.01) \n",
        "\n",
        "      #add 4. row\n",
        "      newrow = [0,0,0,1]\n",
        "      out = np.vstack([out, newrow])\n",
        "      out = np.linalg.inv(out)\n",
        "      #out = np.matmul(out, outPrev)\n",
        "      poses.append(out)\n",
        "      print(out)\n",
        "\n",
        "      #giving the parameters for the next iteration\n",
        "      outPrev = out\n",
        "      imgprevu = imgu\n",
        "      imgprevdptu = imgdptu\n",
        "\n",
        "cntr=0\n",
        "for i in myAnswers:\n",
        "  myAnswers[allDestination[cntr]]['poses']=[poses[cntr][0][0], poses[cntr][0][1], poses[cntr][0][2], poses[cntr][0][3],\n",
        "                                            poses[cntr][1][0], poses[cntr][1][1], poses[cntr][1][2], poses[cntr][1][3],\n",
        "                                            poses[cntr][2][0], poses[cntr][2][1], poses[cntr][2][2], poses[cntr][2][3]]\n",
        "  cntr+=1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HW/g1/rgb/1.jpg\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "HW/g1/rgb/33.jpg\n",
            "matches : 64\n",
            "[[ 1.00235174 -0.00432047  0.04134961  0.10194345]\n",
            " [ 0.00255406  0.98062993 -0.00965156 -0.01854033]\n",
            " [-0.08851469 -0.0057688   1.05798109 -0.03851982]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/60.jpg\n",
            "matches : 54\n",
            "[[ 1.00364557  0.00683762  0.07658473  0.10889177]\n",
            " [ 0.03484582  1.01946267 -0.00814553  0.0160288 ]\n",
            " [-0.04170812  0.04326453  0.9498505   0.02082783]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/89.jpg\n",
            "matches : 70\n",
            "[[ 0.99041557  0.0226098   0.07514501  0.09231288]\n",
            " [-0.01705396  1.01374852 -0.03513311  0.02781189]\n",
            " [-0.01378546  0.0065819   0.90137688  0.07662033]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/123.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:133: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "matches : 61\n",
            "[[ 1.00049817 -0.00305524 -0.05772358  0.09402374]\n",
            " [-0.00539597  1.00539965  0.03196634 -0.0102559 ]\n",
            " [ 0.06775291  0.01349934  0.96635167  0.01190036]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/145.jpg\n",
            "matches : 32\n",
            "[[ 0.99889807 -0.00952812  0.02022752  0.04434172]\n",
            " [-0.00991005  0.98371882 -0.03265787  0.09694245]\n",
            " [ 0.00180216  0.08723445  0.9768252  -0.02934   ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/159.jpg\n",
            "matches : 161\n",
            "[[ 9.86341357e-01 -1.55804119e-03 -1.10558780e-01  8.85849723e-03]\n",
            " [ 8.24674624e-04  1.00818219e+00  6.49477431e-03  1.57341849e-02]\n",
            " [ 1.35078600e-01 -6.66883631e-02  1.03352072e+00 -5.41161024e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g1/rgb/171.jpg\n",
            "matches : 123\n",
            "[[ 0.97366167 -0.0161736  -0.10286014 -0.00310699]\n",
            " [ 0.02663604  0.99558376  0.00489566 -0.00110682]\n",
            " [ 0.11969741 -0.03592643  1.02538499 -0.0358611 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/186.jpg\n",
            "matches : 124\n",
            "[[ 1.00215201 -0.05140587 -0.10297737 -0.01237175]\n",
            " [ 0.02940295  0.98705468  0.04140705 -0.01942728]\n",
            " [ 0.12437756  0.00548776  0.93545352  0.05114426]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/199.jpg\n",
            "matches : 111\n",
            "[[ 1.00782567 -0.00163666 -0.1405641   0.02735213]\n",
            " [ 0.04196478  0.99236789 -0.03609556 -0.00313664]\n",
            " [ 0.10880937  0.08760093  0.90736594  0.07795341]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/213.jpg\n",
            "matches : 89\n",
            "[[ 0.99588612 -0.06024451  0.16286136 -0.09715823]\n",
            " [ 0.03111647  0.98572727 -0.00625677 -0.00142144]\n",
            " [-0.09147701  0.00361864  1.10217072 -0.09854897]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/234.jpg\n",
            "matches : 53\n",
            "[[ 0.99333704  0.01130446  0.08714503 -0.06175867]\n",
            " [ 0.00490262  0.97675097 -0.01457082 -0.0102724 ]\n",
            " [-0.11769552 -0.02873963  1.10253736 -0.05877565]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/258.jpg\n",
            "matches : 64\n",
            "[[ 0.9947426   0.04632521  0.10599769 -0.08444656]\n",
            " [-0.05672198  0.97585633  0.09187571  0.01688787]\n",
            " [-0.10293716 -0.18310905  1.07328814 -0.04447622]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/270.jpg\n",
            "matches : 99\n",
            "[[ 9.89309564e-01  1.11832730e-01  9.77943948e-02 -4.11586102e-02]\n",
            " [-1.06185956e-01  9.87163048e-01 -4.13825310e-02 -9.13872792e-04]\n",
            " [-8.56221473e-02  4.75039545e-02  9.74081919e-01  4.46347660e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g1/rgb/295.jpg\n",
            "matches : 112\n",
            "[[ 0.98812179  0.06675944  0.11056455 -0.03576268]\n",
            " [-0.06961605  1.01087092 -0.11734686  0.02762607]\n",
            " [-0.11333708  0.06763322  0.98354879  0.02582342]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/307.jpg\n",
            "matches : 161\n",
            "[[ 9.99417808e-01  8.46485351e-02  8.52166393e-02  9.15586336e-04]\n",
            " [-6.86690115e-02  1.00481206e+00  4.10901048e-03  1.97634549e-02]\n",
            " [-7.79077061e-02 -2.12491631e-02  9.84041384e-01  2.59204964e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g1/rgb/326.jpg\n",
            "matches : 59\n",
            "[[ 1.00195283 -0.05165036 -0.08694455  0.00202589]\n",
            " [ 0.09180909  1.02759722  0.14740072 -0.06013489]\n",
            " [ 0.08305244 -0.24396951  1.0268966  -0.0088709 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/381.jpg\n",
            "matches : 42\n",
            "[[ 1.04969443  0.08321932 -0.23776678  0.22026929]\n",
            " [-0.03643285  0.91033402  0.12315432 -0.21850716]\n",
            " [-0.11095784 -0.31115193  1.49948255 -0.54053046]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/394.jpg\n",
            "matches : 95\n",
            "[[ 0.99152413 -0.03485369 -0.1041697  -0.03568316]\n",
            " [ 0.02353157  0.98517519  0.01093963 -0.03198164]\n",
            " [ 0.10584419 -0.01588831  1.00351568 -0.0208    ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g1/rgb/411.jpg\n",
            "matches : 102\n",
            "[[ 0.99342021 -0.02607962 -0.11557593 -0.01401978]\n",
            " [ 0.03774079  0.99177229  0.00818685 -0.01133968]\n",
            " [ 0.10052729  0.02140648  0.98665168  0.00430846]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/1.jpg\n",
            "new frameset : 0\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "HW/g2/rgb/47.jpg\n",
            "matches : 20\n",
            "[[ 0.98377829 -0.01421514  0.08183216  0.10637328]\n",
            " [ 0.04737067  1.0049661  -0.00249069  0.01305084]\n",
            " [-0.09413382  0.0045042   0.97753652  0.00365676]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/64.jpg\n",
            "matches : 49\n",
            "[[ 1.03403457  0.00458232  0.04687515  0.13376231]\n",
            " [-0.03003216  1.00893683  0.01792255 -0.04852375]\n",
            " [-0.08171737 -0.01146352  1.00702885 -0.00884454]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/108.jpg\n",
            "matches : 59\n",
            "[[ 1.03546654  0.09300906  0.11823012 -0.01569801]\n",
            " [-0.02655897  0.97554234  0.01788618  0.00989344]\n",
            " [-0.02824353 -0.01174504  1.00979368 -0.03682755]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/126.jpg\n",
            "matches : 73\n",
            "[[ 0.99603586  0.08804117  0.08545275  0.08813501]\n",
            " [-0.05703842  0.89379766  0.03613954 -0.02534603]\n",
            " [-0.04591948 -0.09447688  1.05754269 -0.03154035]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/147.jpg\n",
            "matches : 101\n",
            "[[ 0.99048156  0.06211182 -0.18425771  0.25626654]\n",
            " [-0.07313133  0.97344888 -0.05749296  0.07935606]\n",
            " [-0.02071971 -0.01557585  0.73534547  0.21664751]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/185.jpg\n",
            "matches : 51\n",
            "[[ 1.00126541  0.01199349  0.09076709  0.00986758]\n",
            " [ 0.0330869   1.01678299 -0.08820035  0.05432264]\n",
            " [ 0.0266146  -0.04424706  0.88936349  0.10095826]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/205.jpg\n",
            "matches : 34\n",
            "[[ 0.99260684 -0.05417406 -0.00977582  0.00491748]\n",
            " [ 0.04157478  1.00644302 -0.13015312  0.09067166]\n",
            " [ 0.02246709  0.11882193  1.01400622 -0.060941  ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/236.jpg\n",
            "matches : 56\n",
            "[[ 1.04154047 -0.11246455  0.03670733 -0.21165927]\n",
            " [ 0.06800417  0.96459614  0.0322548  -0.0347833 ]\n",
            " [ 0.08672358  0.03563437  0.97660072  0.01224393]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/257.jpg\n",
            "matches : 31\n",
            "[[ 1.01155246 -0.05725929 -0.00123511 -0.12929712]\n",
            " [ 0.03921459  1.00851619 -0.04660405  0.024405  ]\n",
            " [ 0.02532993  0.0437875   0.99520265  0.02097806]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/286.jpg\n",
            "matches : 19\n",
            "[[ 9.97173441e-01 -9.44277144e-02 -6.51195118e-02 -1.16560922e-01]\n",
            " [ 3.61405083e-02  9.53545667e-01  4.46749670e-03  6.36382524e-04]\n",
            " [ 4.85425769e-02 -5.08773474e-02  1.06405101e+00 -1.63378812e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g2/rgb/313.jpg\n",
            "matches : 54\n",
            "[[ 1.02473742e+00 -4.55306195e-02 -4.96977832e-02 -8.07932548e-02]\n",
            " [ 2.26756211e-02  1.01388824e+00  4.04624443e-05  1.87613481e-02]\n",
            " [ 2.06032965e-03  2.04591960e-02  9.71553734e-01  8.07859640e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g2/rgb/329.jpg\n",
            "matches : 85\n",
            "[[ 0.99106338  0.01190199  0.16619468 -0.08921343]\n",
            " [-0.04692382  0.99758194 -0.01258253 -0.00832122]\n",
            " [-0.09268205 -0.03195041  1.07153472 -0.08364032]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/359.jpg\n",
            "matches : 92\n",
            "[[ 0.98619726  0.03819643  0.14369962 -0.04168664]\n",
            " [-0.07903609  0.9905622   0.04800684 -0.04212902]\n",
            " [-0.08993691 -0.02687704  1.03745897 -0.14406832]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/383.jpg\n",
            "matches : 52\n",
            "[[ 0.98785992  0.08783855  0.07919954  0.12296773]\n",
            " [-0.04129337  0.99991311 -0.06840629  0.01117763]\n",
            " [-0.09588239  0.04759487  1.03109543 -0.0249856 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g2/rgb/415.jpg\n",
            "matches : 71\n",
            "[[ 0.99987108  0.01024065  0.08184933  0.07760084]\n",
            " [-0.02928554  1.02311693 -0.00750155  0.04437297]\n",
            " [-0.05075341  0.02211252  0.86933195  0.1359091 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/1.jpg\n",
            "new frameset : 0\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "HW/g3/rgb/65.jpg\n",
            "matches : 63\n",
            "[[ 1.00222697 -0.17344025  0.09173685  0.04914323]\n",
            " [-0.00188746  0.94323221  0.02440854 -0.03001054]\n",
            " [ 0.02067573 -0.14365249  1.08490147 -0.03243077]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/77.jpg\n",
            "matches : 78\n",
            "[[ 0.9946609   0.02321695  0.09432319  0.04503659]\n",
            " [ 0.01167208  0.97431505  0.00513399 -0.00371889]\n",
            " [-0.09124518 -0.02262446  1.01385577 -0.00108979]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/136.jpg\n",
            "matches : 67\n",
            "[[ 1.00169648  0.02715709 -0.02385752  0.1072859 ]\n",
            " [-0.00409395  0.97251499 -0.04963849  0.01890818]\n",
            " [ 0.03573245  0.01893291  1.01025393 -0.0115368 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/153.jpg\n",
            "matches : 58\n",
            "[[ 1.00393281 -0.08999003  0.02856336  0.14819247]\n",
            " [ 0.04287942  0.97933427  0.03471844 -0.0220208 ]\n",
            " [-0.08773633 -0.05675791  1.02519413 -0.0068173 ]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/189.jpg\n",
            "matches : 26\n",
            "[[ 1.00435544  0.04569834  0.01588162  0.10185975]\n",
            " [-0.03089488  1.02809688  0.0387988  -0.01511604]\n",
            " [-0.00531804 -0.01833499  1.06127134 -0.04831297]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/208.jpg\n",
            "matches : 37\n",
            "[[ 0.98848033  0.10829207  0.06260427  0.11041372]\n",
            " [-0.07420543  0.98283459  0.07075878 -0.00724979]\n",
            " [-0.01507378 -0.02525262  1.01880055  0.01680214]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/225.jpg\n",
            "matches : 68\n",
            "[[ 0.99752288  0.00652028 -0.11296504  0.05622953]\n",
            " [ 0.02357569  0.94446735  0.06502643 -0.00565808]\n",
            " [ 0.20142606 -0.35870771  1.18984733 -0.12650989]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/250.jpg\n",
            "matches : 40\n",
            "[[ 9.86153636e-01  1.00404006e-04 -5.40002598e-02  6.68659762e-02]\n",
            " [-9.27471546e-03  1.01005982e+00 -1.19549850e-01  2.90918766e-02]\n",
            " [ 5.32554706e-02  1.05242960e-01  9.89814195e-01 -3.45271424e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g3/rgb/278.jpg\n",
            "matches : 29\n",
            "[[ 0.98912188  0.02019403  0.10857282 -0.03691639]\n",
            " [ 0.00509814  1.00211859 -0.06925781  0.05635943]\n",
            " [-0.10300173  0.03901333  0.99740143 -0.02573346]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/292.jpg\n",
            "matches : 23\n",
            "[[ 0.99946032 -0.05827301 -0.10274886 -0.08656393]\n",
            " [ 0.05196322  0.98378265 -0.03333035 -0.00619074]\n",
            " [ 0.09361507  0.06151468  0.96184368  0.01242438]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/307.jpg\n",
            "matches : 35\n",
            "[[ 0.99069653  0.05216348  0.1233469  -0.29312364]\n",
            " [ 0.07997777  1.03514603 -0.00313468 -0.00344258]\n",
            " [ 0.09224555  0.15861697  1.14067218 -0.14990615]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/327.jpg\n",
            "matches : 41\n",
            "[[ 1.03324733e+00 -7.57832637e-02 -7.81160855e-02 -9.64826170e-02]\n",
            " [-1.05111437e-02  1.09227454e+00  4.50334608e-02 -5.48934238e-02]\n",
            " [ 8.07960844e-04  1.82492147e-02  1.00842858e+00  8.35479366e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g3/rgb/340.jpg\n",
            "matches : 75\n",
            "[[ 1.00092009 -0.06353414 -0.09426785 -0.06407262]\n",
            " [ 0.04369291  1.03177923  0.01820329 -0.00177223]\n",
            " [ 0.08650365  0.07488923  0.93211792  0.03507092]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/356.jpg\n",
            "matches : 86\n",
            "[[ 0.99597158  0.02182669  0.1383382  -0.11042057]\n",
            " [-0.01928367  0.98078643 -0.02388141  0.04717126]\n",
            " [-0.11029251 -0.02860676  1.01640122  0.00824124]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/376.jpg\n",
            "matches : 103\n",
            "[[ 0.99305741 -0.03461882  0.10281175 -0.04156519]\n",
            " [ 0.04240978  0.99906492 -0.03946235 -0.00138386]\n",
            " [-0.10957381  0.0068126   1.00804104 -0.09652516]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/409.jpg\n",
            "matches : 60\n",
            "[[ 0.99964803  0.01857509 -0.01269837  0.10644421]\n",
            " [-0.01195768  1.01312845  0.04038703 -0.00887408]\n",
            " [ 0.00233736 -0.03683425  1.00338729 -0.05897109]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/420.jpg\n",
            "matches : 51\n",
            "[[ 0.98632822  0.01238985  0.11626938  0.04134847]\n",
            " [-0.0628626   1.00320798 -0.00606033  0.01561626]\n",
            " [-0.12920886 -0.02631013  1.01320631  0.00167785]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g3/rgb/441.jpg\n",
            "matches : 35\n",
            "[[ 1.01186523  0.05395373 -0.05720202  0.14191391]\n",
            " [-0.06073233  1.0060002   0.08006867 -0.02197972]\n",
            " [-0.04572504 -0.26230915  0.90569992  0.13719807]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/1.jpg\n",
            "new frameset : 6\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]]\n",
            "HW/g4/rgb/38.jpg\n",
            "matches : 125\n",
            "[[ 1.00256918 -0.05687274  0.10436885  0.01768368]\n",
            " [ 0.0404239   0.99705114  0.03059218  0.00124011]\n",
            " [-0.09114052 -0.00142746  0.97151543  0.03526984]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/62.jpg\n",
            "matches : 24\n",
            "[[ 1.01038125e+00 -1.44555500e-02  1.53626194e-02  9.07915351e-02]\n",
            " [ 6.70259314e-04  9.89052984e-01 -1.90069843e-02  5.22678512e-04]\n",
            " [ 5.87486431e-03  5.39574769e-02  9.74891404e-01  4.37383379e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g4/rgb/78.jpg\n",
            "matches : 75\n",
            "[[ 0.99237451  0.01572755  0.039692    0.12203579]\n",
            " [ 0.01149187  1.0364141  -0.05190493  0.01594247]\n",
            " [-0.07755484  0.05022767  0.9757585   0.02181757]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/95.jpg\n",
            "matches : 76\n",
            "[[ 0.98044584  0.02408782 -0.01193914  0.10297242]\n",
            " [-0.04735119  0.98885492  0.03472358  0.0045121 ]\n",
            " [-0.00610938 -0.05546823  1.00854377  0.00459429]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/116.jpg\n",
            "matches : 62\n",
            "[[ 1.0403829  -0.03265721  0.09175329  0.10511825]\n",
            " [-0.0328595   0.98043275 -0.009382    0.00517416]\n",
            " [-0.04743909  0.09930378  0.93103295  0.04516101]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/134.jpg\n",
            "matches : 40\n",
            "[[ 1.0242515   0.00602546  0.00166543  0.07539881]\n",
            " [ 0.00995405  0.99817901  0.03271489 -0.0173911 ]\n",
            " [ 0.11490539 -0.05299827  1.19000561 -0.12916033]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/152.jpg\n",
            "matches : 89\n",
            "[[ 9.96294930e-01  5.12893761e-02 -3.38293952e-02  1.04336387e-01]\n",
            " [-7.14017926e-02  1.00788269e+00  3.48048182e-02 -2.65441317e-04]\n",
            " [ 3.62255147e-02  1.06644607e-02  9.71660308e-01  1.17120566e-02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g4/rgb/159.jpg\n",
            "matches : 135\n",
            "[[ 9.95800326e-01 -7.57954375e-02 -8.49348332e-02 -1.88286157e-18]\n",
            " [ 2.60754371e-02  9.82007561e-01  3.46229122e-02 -1.59552083e-17]\n",
            " [ 1.02145230e-01 -3.06337135e-02  9.96675455e-01 -1.46608438e-17]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g4/rgb/190.jpg\n",
            "matches : 91\n",
            "[[ 0.98458292  0.00569816 -0.01943329  0.01603069]\n",
            " [ 0.02745937  0.98310281 -0.09175864  0.00231765]\n",
            " [ 0.03513696  0.04300206  1.04408971 -0.01607756]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/210.jpg\n",
            "matches : 13\n",
            "[[ 0.99301342  0.0679232   0.02611075 -0.11796032]\n",
            " [-0.01145768  0.98842087 -0.02979445  0.00533918]\n",
            " [-0.05337617  0.1172288   1.15124116 -0.03479548]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/229.jpg\n",
            "matches : 15\n",
            "[[ 1.00693502 -0.0606459   0.05477554 -0.15612506]\n",
            " [ 0.0219276   1.02939862  0.11206582 -0.07576373]\n",
            " [ 0.03734692 -0.14724185  1.15196259 -0.06023506]\n",
            " [ 0.          0.          0.          1.        ]]\n",
            "HW/g4/rgb/250.jpg\n",
            "matches : 31\n",
            "[[ 9.76414639e-01 -1.78127035e-01 -2.76622564e-01 -1.97844070e-17]\n",
            " [ 4.18839602e-02  9.63112297e-01  2.77672537e-02  4.93702968e-18]\n",
            " [-3.37613320e-02 -6.59505486e-02  1.01698653e+00  7.12799534e-17]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g4/rgb/270.jpg\n",
            "matches : 22\n",
            "[[ 9.24324049e-01 -1.61417079e-01 -2.98208564e-01 -3.68126902e-18]\n",
            " [ 3.00536404e-02  1.01960656e+00 -1.76678978e-02 -1.44771703e-18]\n",
            " [ 3.01835682e-02 -1.72102570e-02  1.04047963e+00  7.96932056e-17]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
            "HW/g4/rgb/297.jpg\n",
            "matches : 17\n",
            "[[ 1.00797532 -0.09096366  0.03050336 -0.11967876]\n",
            " [ 0.0332113   1.05644306  0.01048505  0.00108968]\n",
            " [ 0.03770806 -0.16123738  1.06089578 -0.02708531]\n",
            " [ 0.          0.          0.          1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci2jK2ckyU47",
        "colab_type": "text"
      },
      "source": [
        "**HowTo**: Run all the block excluding the **Main Loop** and the ones after. **Main Loop** is for training the network. It's already done, and the best network is donwloaded from the Git project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juaSo-TdBVjH",
        "colab_type": "text"
      },
      "source": [
        "**Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7HjZZb3_Oak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports ----------------------------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, transform\n",
        "from PIL import Image # for Image.open()\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "# Classes ------------------------------------------------------------------\n",
        "classes = subclassNames[0]\n",
        "classes_num = len(classes) # Number of classes to classify\n",
        "#print(classes_num)\n",
        "\n",
        "# Hyper parameters -------------------------------------------------------------\n",
        "batch_size=128\n",
        "numEpoch = 100\n",
        "patience = 10 # Patience parameter for Early Stopping: if the best accuracy is not updated in \"patience\" steps, Main Loop is stopped (avoiding overfitting)\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# Utilizing the GPU\n",
        "haveCuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sldh7dvNA4Cs",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional Module + Network Def**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBhjC17ZA2rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional module (Conv+ReLU+BatchNorm) -----------------------------------\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super(Conv, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=1, bias=False)        \n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return self.bn(torch.relu(self.conv(x)))\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, base_channels=16, in_channels=3, num_classes=classes_num):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        self.c11 = Conv(in_channels, base_channels)\n",
        "        self.c12 = Conv(base_channels, base_channels)   \n",
        "        self.d1 = Conv(base_channels, base_channels*2, 2) # Downscale using strided convolution and expand channels\n",
        "        \n",
        "        self.c21 = Conv(base_channels*2, base_channels*2)\n",
        "        self.c22 = Conv(base_channels*2, base_channels*2)     \n",
        "        self.d2 = Conv(base_channels*2, base_channels*4, 2)\n",
        "        \n",
        "        self.c31 = Conv(base_channels*4, base_channels*4)\n",
        "        self.c32 = Conv(base_channels*4, base_channels*4)       \n",
        "        self.d3 = Conv(base_channels*4, base_channels*8, 2)\n",
        "        \n",
        "        self.c41 = Conv(base_channels*8, base_channels*8)\n",
        "        self.c42 = Conv(base_channels*8, base_channels*8)       \n",
        "        self.d4 = Conv(base_channels*8, base_channels*16, 2)\n",
        "        \n",
        "        self.c51 = Conv(base_channels*16, base_channels*16)\n",
        "        self.c52 = Conv(base_channels*16, base_channels*16)           \n",
        "        self.d5 = Conv(base_channels*16, base_channels*32, 2) # Input image is 32x32 -> after 5 downscaling the activation map is 1x1\n",
        "        \n",
        "        # Classifier is a normal 1x1 convolution that produces num_classes class scores\n",
        "        # This layer does not have BatchNorm of ReLU\n",
        "        self.classifier = nn.Conv2d(base_channels*32,num_classes,kernel_size=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.d1(self.c12(self.c11(x)))\n",
        "        x = self.d2(self.c22(self.c21(x)))\n",
        "        x = self.d3(self.c32(self.c31(x)))\n",
        "        x = self.d4(self.c42(self.c41(x)))\n",
        "        x = self.d5(self.c52(self.c51(x)))\n",
        "        \n",
        "        # Squeeze removes dimensions that have only 1 element\n",
        "        # Output of the conv layer is (batch_size x num_classes x 1 x 1)\n",
        "        # After squeeze is becomes (batch_size x num_classes)\n",
        "        return torch.squeeze(self.classifier(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTKOkaxwBHpi",
        "colab_type": "text"
      },
      "source": [
        "**Transformation + Data Augmentation + Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roFF15RZBG-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformations + Data Augmentation for the training data -------------------- \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    # Transformations regarding the image position (crop, flip, affin transform)\n",
        "    transforms.RandomCrop(32,padding=4), # Random 32x32 crops (with 4-wide zero padding - this is needed because the input is 32x32 so we can't crop a 32x32 region out of it without padding)\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    #transforms.RandomAffine(degrees=10, shear=50),\n",
        "    #transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # Transformations regarding the image data\n",
        "    transforms.ColorJitter(brightness=0.8,contrast=0.3,saturation=0.3,hue=0.2),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    # The following 2 transformations are a must\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "# Data loading -----------------------------------------------------------------\n",
        "data_path_train = 'trafficSignsHW/trainFULL'\n",
        "data_path_test = 'trafficSignsHW/testFULL'\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(root=data_path_train, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root=data_path_test, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSTwqwQyBlx_",
        "colab_type": "text"
      },
      "source": [
        "**Function for creating things + Display**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsZv6GaTBk9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate network + convert it to CUDA -------------------------------------\n",
        "def createNet():\n",
        "    net = ConvNet()\n",
        "    if haveCuda:\n",
        "        net = net.cuda()\n",
        "    return net\n",
        "\n",
        "# Fucntion for creating the Loss Function --------------------------------------\n",
        "def createLoss():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "# Function for creating the Optimizer ------------------------------------------\n",
        "def createOptimizer():\n",
        "    return optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
        "\n",
        "# Cosine annealing learning rate scheduler - in 50 epochs the lr will become 0.01\n",
        "def createScheduler():\n",
        "    return optim.lr_scheduler.CosineAnnealingLR(optimizer,numEpoch,eta_min=1e-2)\n",
        "\n",
        "# Display ----------------------------------------------------------------------\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGNzWSIYB1OH",
        "colab_type": "text"
      },
      "source": [
        "**Train Function (1 epoch)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYmkLj9FB0aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for training a single epoch -----------------------------------------\n",
        "def train(epoch):\n",
        "    # variables for loss\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "   \n",
        "    net.train() # set the network to train (for batchnorm and dropout)\n",
        "\n",
        "    bar = display(progress(0, len(train_loader)), display_id=True) # Create progress bar\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader, 0): # inputs == one minibatch of images, inputs == corresponding labels\n",
        "        # Convert to cuda\n",
        "        if haveCuda:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad() # Clear any previous gradients (otherwise it would accumulate)\n",
        "\n",
        "        outputs = net(images) # Forward: Calculating the Network's Output for every image in a batch (e.g. for 128 images there will be a 128 x classes_num tensor, each row representing an image and each column is the \"goodnes value\" of the class in the given index) \n",
        "        loss = criterion(outputs, labels) # Loss    \n",
        "        loss.backward() # Backpropagation      \n",
        "        optimizer.step() # Gradient method\n",
        "\n",
        "        # Do not include these steps in the computational graph (otherwise gradient would be calculated)\n",
        "        with torch.no_grad():\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item()        \n",
        "            _, predicted = torch.max(outputs, 1) # Get indices of the largest goodness values: For every image we will get a number, the index of the class for which the \"goodness value\" was the greatest\n",
        "            correct += predicted.eq(labels).sum().item() # Count how many of the predictions equal the labels\n",
        "            total += labels.size(0) # Accumulate number of total images seen \n",
        "\n",
        "        \"\"\"\n",
        "        if (i % 100 == 0):\n",
        "              print(f'labels: {labels}')\n",
        "              print(f'output: {outputs}')\n",
        "              print(f'predicted: {predicted}, correct: {correct}, total: {total}')\n",
        "        \"\"\"\n",
        "      \n",
        "        bar.update(progress(i+1, len(train_loader))) \n",
        "\n",
        "    # Return loss and accuracy\n",
        "    tr_loss = running_loss / i\n",
        "    tr_corr = correct / total * 100\n",
        "    print(\"Train epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, tr_corr))\n",
        "    return tr_loss,tr_corr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nd0BCQbB8Tj",
        "colab_type": "text"
      },
      "source": [
        "**Validation Function (1 epoch)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiZYxhmyB7om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for validating a single epoch ---------------------------------------\n",
        "def val(epoch):\n",
        "    # variables for loss\n",
        "    running_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0\n",
        "   \n",
        "    net.eval() # set the network to eval  (for batchnorm and dropout)\n",
        "\n",
        "    bar = display(progress(0, len(test_loader)), display_id=True) # Create progress bar\n",
        "\n",
        "    for i, (images, labels) in enumerate(test_loader, 0):\n",
        "        # Convert to cuda\n",
        "        if haveCuda:\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        # Do not include these steps in the computational graph (otherwise gradient would be calculated)\n",
        "        with torch.no_grad():         \n",
        "            outputs = net(images) # Forward\n",
        "            loss = criterion(outputs, labels) # Loss\n",
        "\n",
        "            # Compute statistics, just like before\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            \n",
        "\n",
        "        bar.update(progress(i+1, len(test_loader))) # Progress bar update\n",
        "\n",
        "    # Return loss and accuracy\n",
        "    val_loss = running_loss / i\n",
        "    val_corr = correct / total * 100\n",
        "    print(\"Test epoch %d loss: %.3f correct: %.2f\" % (epoch + 1, running_loss / i, val_corr))\n",
        "    return val_loss,val_corr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXxL60x8CJus",
        "colab_type": "text"
      },
      "source": [
        "Main Loop (do not run this!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ64HT2aCJCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9f9a57fb-edf2-4803-ac0b-1f8b94890d12"
      },
      "source": [
        "\"\"\"\n",
        "# Containers for losses and accuracies for every epoch -------------------------\n",
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "\n",
        "best_acc = 0 # Best validation accuracy (this will be updated once a greater acc is found + the model will be saved)\n",
        "\n",
        "# Set pseudo-random generator seeds to make multiple runs comparable -----------\n",
        "torch.manual_seed(1)\n",
        "if haveCuda:\n",
        "    torch.cuda.manual_seed(1)\n",
        "\n",
        "# Create net, criterion, optimizer and scheduler -------------------------------\n",
        "net = createNet()\n",
        "criterion = createLoss()\n",
        "optimizer = createOptimizer()\n",
        "scheduler = createScheduler()\n",
        "wait = 0 # Counter variable for Early Stopping, if \"wait\" reaches \"patience\", Main Loop is stopped\n",
        "\n",
        "# Main Loop --------------------------------------------------------------------\n",
        "for epoch in range(numEpoch):\n",
        "    scheduler.step() # LR scheduler\n",
        "    \n",
        "    # Train\n",
        "    loss,acc = train(epoch) # acc == accurate\n",
        "    train_accs.append(acc)\n",
        "    train_losses.append(loss)\n",
        "    \n",
        "    # Validate\n",
        "    loss,acc = val(epoch) # acc == accurate\n",
        "    val_accs.append(acc)\n",
        "    val_losses.append(loss)\n",
        "    \n",
        "    # Saving the model\n",
        "    if acc > best_acc:\n",
        "        print(\"Best Model, Saving\")\n",
        "        best_acc = acc\n",
        "        torch.save(net.state_dict(),\"model.pth\")\n",
        "        wait = 0\n",
        "    else:\n",
        "      wait += 1\n",
        "      if wait == patience:\n",
        "        print(f'Patience reached, Early Stopping at Epoch {epoch+1}')\n",
        "        break\n",
        "\"\"\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Containers for losses and accuracies for every epoch -------------------------\\ntrain_accs = []\\ntrain_losses = []\\nval_accs = []\\nval_losses = []\\n\\nbest_acc = 0 # Best validation accuracy (this will be updated once a greater acc is found + the model will be saved)\\n\\n# Set pseudo-random generator seeds to make multiple runs comparable -----------\\ntorch.manual_seed(1)\\nif haveCuda:\\n    torch.cuda.manual_seed(1)\\n\\n# Create net, criterion, optimizer and scheduler -------------------------------\\nnet = createNet()\\ncriterion = createLoss()\\noptimizer = createOptimizer()\\nscheduler = createScheduler()\\nwait = 0 # Counter variable for Early Stopping, if \"wait\" reaches \"patience\", Main Loop is stopped\\n\\n# Main Loop --------------------------------------------------------------------\\nfor epoch in range(numEpoch):\\n    scheduler.step() # LR scheduler\\n    \\n    # Train\\n    loss,acc = train(epoch) # acc == accurate\\n    train_accs.append(acc)\\n    train_losses.append(loss)\\n    \\n    # Validate\\n    loss,acc = val(epoch) # acc == accurate\\n    val_accs.append(acc)\\n    val_losses.append(loss)\\n    \\n    # Saving the model\\n    if acc > best_acc:\\n        print(\"Best Model, Saving\")\\n        best_acc = acc\\n        torch.save(net.state_dict(),\"model.pth\")\\n        wait = 0\\n    else:\\n      wait += 1\\n      if wait == patience:\\n        print(f\\'Patience reached, Early Stopping at Epoch {epoch+1}\\')\\n        break\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZZ2ARa1MxB3",
        "colab_type": "text"
      },
      "source": [
        "Function for classifying a single image + Loading 1 image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziqb8YmKMwJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "imsize = 32\n",
        "loader = transforms.Compose([transforms.Resize(imsize),\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))                           \n",
        "                             ])\n",
        "\n",
        "def imageLoader(path):\n",
        "    #image = io.imread(path)\n",
        "    image = Image.open(path)\n",
        "    image = loader(image).float()\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "def imageClassifier(net, image):\n",
        "  with torch.no_grad():         \n",
        "    outputs = net(image) # Forward\n",
        "    print(outputs)\n",
        "    print(outputs.sum())\n",
        "    predicted_value, predicted_label = torch.max(outputs, -1)\n",
        "\n",
        "  return predicted_value, predicted_label\n",
        "\n",
        "img = imageLoader('TestFiles/Potatoes/walk_2.jpg')\n",
        "a, b = imageClassifier(net, img)\n",
        "print(a, b)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwE8UUgCCU4b",
        "colab_type": "text"
      },
      "source": [
        "Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E54QCO0WCUdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "%matplotlib inline\n",
        "\n",
        "# X coordinate for plotting\n",
        "x = np.arange(numEpoch)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# Train is red, validation is blue\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(x,train_accs,'r')\n",
        "plt.plot(x,val_accs,'b')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(x,train_losses,'r')\n",
        "plt.plot(x,val_losses,'b')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Get a minibatch from the test loader and convert to cuda\n",
        "inputs, labels = next(iter(test_loader))\n",
        "if haveCuda:\n",
        "    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "# forward\n",
        "outputs = net(inputs)\n",
        "\n",
        "# Get predicted class indices\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Values used for normalization\n",
        "mean = torch.Tensor((0.49139968, 0.48215827, 0.44653124)).unsqueeze(1).unsqueeze(1)\n",
        "std = torch.Tensor((0.24703233, 0.24348505, 0.26158768)).unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "# List of subplots - we'll use 16 images\n",
        "f, axarr = plt.subplots(2, 8,figsize=(20, 5))\n",
        "\n",
        "# For every image-prediction pair\n",
        "for i,(img,pred) in enumerate(zip(inputs,predicted)):\n",
        "    # undo the normalization\n",
        "    img_rescaled = img.cpu() * std + mean\n",
        "    \n",
        "    # Get predicted class name\n",
        "    name = classes[pred.cpu().item()]\n",
        "    \n",
        "    # Permutation needed because in PyTorch the channel dimension comes first,\n",
        "    # but in numpy and opencv it comes last (3x32x32) -> (32x32x3)\n",
        "    axarr[i//8,i%8].imshow(img_rescaled.permute(1,2,0))\n",
        "    \n",
        "    # Set title to class name\n",
        "    axarr[i//8,i%8].set_title(name)\n",
        "    \n",
        "    # Hide grid lines\n",
        "    axarr[i//8,i%8].grid(False)\n",
        "    \n",
        "    # Hide axes ticks\n",
        "    axarr[i//8,i%8].set_xticks([])\n",
        "    axarr[i//8,i%8].set_yticks([])\n",
        "    \n",
        "    # Only do the first 16\n",
        "    if i == 15:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnBE_fq7MTQL",
        "colab_type": "text"
      },
      "source": [
        "Classification Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9pYzKUxMRbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "#Loading an image\n",
        "from skimage import io, transform\n",
        "\n",
        "image = io.imread('trafficSignsHW/testFULL/Bump/img(8200).jpg')\n",
        "\n",
        "# Loading the previously saved model\n",
        "my_net = createNet()\n",
        "loaded_state_dict = torch.load('model.pth')\n",
        "my_net.load_state_dict(loaded_state_dict)\n",
        "\n",
        "value, label = ImageClassify(my_net, image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1kysXYa7B-7",
        "colab_type": "text"
      },
      "source": [
        "Single Image Transform Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oIQS15s7BUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "imsize = 32\n",
        "loader = transforms.Compose([transforms.Resize(imsize),\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))                           \n",
        "                             ])\n",
        "\n",
        "def image_loader(path):\n",
        "    image = Image.open(path)\n",
        "    image = loader(image).float()\n",
        "    image = image.unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "path = 'trafficSignsHW/testFULL/Bump/img(8200).jpg'\n",
        "image = image_loader(path)\n",
        "\n",
        "outputs = net(image)\n",
        "adss, predicted = torch.max(outputs, -1)\n",
        "print(adss, predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQvWCxnP_Pbr",
        "colab_type": "text"
      },
      "source": [
        "# **Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la_WSnnChirk",
        "colab_type": "text"
      },
      "source": [
        "#Teszt-Balage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FDLthQb6nzh",
        "colab_type": "text"
      },
      "source": [
        "This part estimates the correct class for the images given by Andris in Task 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8kgbh9vhiOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the best Net ---------------------------------------------------------\n",
        "loaded_state_dict = torch.load('model_Final.pth')\n",
        "net_loaded = createNet()\n",
        "net_loaded.load_state_dict(loaded_state_dict)\n",
        "\n",
        "# Single Image Transform -------------------------------------------------------\n",
        "imsize = 32\n",
        "loader = transforms.Compose([torchvision.transforms.ToPILImage(),\n",
        "                             transforms.Resize(imsize),\n",
        "                             transforms.CenterCrop(32),\n",
        "                             transforms.ToTensor(),                        \n",
        "                             ])\n",
        "\n",
        "# Single Image Classifier ------------------------------------------------------\n",
        "def imageClassifier(net, image):\n",
        "  with torch.no_grad():\n",
        "    net.eval()       \n",
        "    outputs = net(image) # Forward\n",
        "    predicted_value, predicted_label = torch.max(outputs, -1)\n",
        "\n",
        "  return predicted_value, predicted_label\n",
        "\n",
        "# Work -------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "cntr=0\n",
        "for i in imgsRgb:\n",
        "  plt.subplot(24,3,cntr+1)\n",
        "  plt.imshow(drawBBs(myAnswers[allDestination[cntr]][\"objects\"], i))\n",
        "  cntr+=1\n",
        "  if(cntr==20):\n",
        "    break\n",
        "\"\"\"\n",
        "cntr=0\n",
        "cntr1=0\n",
        "\n",
        "# Loops over all the images\n",
        "for i in imgsRgb: \n",
        "  objectX=myAnswers[allDestination[cntr]][\"objects\"]\n",
        "\n",
        "  cntr2=0\n",
        "  for j in objectX:\n",
        "    if(j[4]==0):\n",
        "\n",
        "      imgCpy=np.copy(i)\n",
        "      #plt.imshow(imgCpy)\n",
        "\n",
        "      Cropped=imgCpy[max((j[1]-j[3]//2),0):(j[1]+j[3]//2),max((j[0]-j[2]//2),0):(j[0]+j[2]//2)]\n",
        "\n",
        "      #cv2.imwrite('AndrisImages/'+str(cntr)+'.jpg', Cropped)   \n",
        "      #plt.subplot(5,3,cntr1+1)\n",
        "      #plt.imshow(Cropped)\n",
        "\n",
        "      cntr1+=1\n",
        "\n",
        "      Cropped = loader(Cropped).float()\n",
        "      Cropped = Cropped.unsqueeze(0)\n",
        "      Cropped = Cropped.cuda()\n",
        "\n",
        "      a, b = imageClassifier(net_loaded, Cropped) # a: predicted value, b: predicted class\n",
        "      correct_class = b\n",
        "\n",
        "      myAnswers[allDestination[cntr]][\"objects\"][cntr2][5]=correct_class\n",
        "\n",
        "    cntr2+=1\n",
        "  cntr+=1\n",
        "\"\"\"\n",
        "  if(cntr==20):\n",
        "    break\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EDSRw6q7kdB",
        "colab_type": "text"
      },
      "source": [
        "Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjlICK9e60a8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ40v5go7atC",
        "colab_type": "text"
      },
      "source": [
        "Visualizing EVERYTING!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fW-D6ErUm5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(30,300))\n",
        "\n",
        "cntr=0\n",
        "for i in imgsRgb:\n",
        "  if(cntr<20):\n",
        "    cntr+=1\n",
        "    continue\n",
        "  plt.subplot(24,2,cntr+1)\n",
        "  plt.imshow(drawBBs(myAnswers[allDestination[cntr]][\"objects\"], i))\n",
        "  cntr+=1\n",
        "  if(cntr==40):\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZmTz3XRdwm1",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This snippet assumes that the contents of the downloaded zip file are in the HW folder, and that your predictions are in a dictionary called predictions that adheres to the format specified above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRADdkYd0Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from HW.evaluate import evaluate\n",
        "file = open('HW/annotations.pickle','rb')\n",
        "predictions = pickle.load(file)\n",
        "predictions=myAnswers\n",
        "evaluate(predictions)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}